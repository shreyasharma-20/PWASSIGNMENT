{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386f6094-226f-4af2-bc55-754c68ada2c8",
   "metadata": {},
   "source": [
    "# Feature Engineering Assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9bccf-8c74-4acb-b85c-ce2bfda46ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e1f72a9-3416-4640-ac0e-15caae00be6a",
   "metadata": {},
   "source": [
    "# Assignment Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359abfda-bb5f-4c65-8cde-4993ec73730e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77882cd1-791f-479d-a4e0-1a0460c9f5ef",
   "metadata": {},
   "source": [
    "Q1) What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e55dcd-94f9-437d-805d-852f8a0e74b9",
   "metadata": {},
   "source": [
    "Ans.  In machine learning, a parameter refers to a variable within a model that is learned from the data during the training process.     \n",
    "      Parameters determine how the model makes predictions based on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115dfe6e-292d-4fcc-8f0f-a5e63afb4343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca9a805a-3a4d-4c45-85a4-0e3c76a99cba",
   "metadata": {},
   "source": [
    "Q2) What is correlation?\n",
    "    What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d244dee9-25ac-468d-a388-d92bed0c1f3b",
   "metadata": {},
   "source": [
    "Ans. Correlation is a statistical measure that describes the relationship between two variables. \n",
    "     It indicates how one variable changes in relation to another and is expressed as a value between -1 and +1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bba32-8fce-4630-8418-d2845edd9f8b",
   "metadata": {},
   "source": [
    "Negative correlation means that as one variable increases, the other variable decreases, and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70010b10-0e1d-4382-9e3c-19139227a1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc10f0e9-6132-424f-8e0f-d73f782bf701",
   "metadata": {},
   "source": [
    "Q3) Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f0ac1-6eca-42f9-ae2d-772956899dd6",
   "metadata": {},
   "source": [
    "Ans. Machine Learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems capable of learning and improving from experience without being explicitly programmed. It uses algorithms and statistical models to identify patterns in data and make predictions or decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4b2b8-f5ab-4716-89ad-11baa9d83a45",
   "metadata": {},
   "source": [
    "The main components of machine learning are Data, Features, Model, Algorithm, Objective/Loss Function, Optimization, Evaluation, and Prediction/Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b4a60-3c7d-441b-96f0-ccb8fff50947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c99ed96-1fd7-4872-9501-dcdccc81dac2",
   "metadata": {},
   "source": [
    "Q4) How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234daa5-c1a2-4dad-80dd-7cb616b6d7be",
   "metadata": {},
   "source": [
    "Ans. The loss value shows how far the model's predictions are from actual targets. A low loss means good performance, while a high loss indicates poor predictions. It helps detect underfitting/overfitting, guides training, and evaluates generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3699abf-6c6a-4c26-9f81-deb8e70ca7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704ca489-21d2-4e69-973d-e71ad05746ac",
   "metadata": {},
   "source": [
    "Q5) What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b476d19-fd22-4de7-ad18-70a06202f879",
   "metadata": {},
   "source": [
    "Ans. Continuous Variables: These are variables that can take any value within a range, often representing measurable quantities. Examples include height, weight, and temperature. They are typically represented by real numbers and allow for mathematical operations like addition and division."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281489e-8658-4b7a-bc82-8c665a8c4031",
   "metadata": {},
   "source": [
    "Categorical Variables: These variables represent distinct categories or groups. They can be nominal (unordered, e.g., gender, colors) or ordinal (ordered, e.g., education levels). Categorical variables are usually represented by labels or codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9554fa-125f-4873-bfdc-f33d48aba582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9650f72a-74cc-436a-bc14-1839f31a7789",
   "metadata": {},
   "source": [
    "Q6) How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e773c9fa-6113-41c0-8d48-34faa526f5c9",
   "metadata": {},
   "source": [
    "Ans. To handle categorical variables in machine learning, common techniques include:\n",
    "\n",
    " 1. Label Encoding: Assigns unique integers to categories (best for ordinal data).\n",
    " 2. One-Hot Encoding: Creates binary columns for each category (best for nominal data).\n",
    " 3. Binary Encoding: Combines label encoding and binary conversion, reducing dimensionality.\n",
    " 4. Frequency Encoding: Replaces categories with their frequency in the dataset.\n",
    " 5. Target Encoding: Replaces categories with the mean of the target variable.\n",
    " 6. Embedding Layers: Used in neural networks to map categories to dense vectors.\n",
    " 7. Hash Encoding: Maps categories using a hash function, reducing dimensionality.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8b754-072a-42a7-a5a6-0313bf188d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59591d64-01a3-43e0-a2ac-08cdc1564de5",
   "metadata": {},
   "source": [
    "Q7) What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722c622-d737-49c0-ba7c-3d22f4569d6e",
   "metadata": {},
   "source": [
    "Ans 1. Training a Dataset: This involves feeding a portion of the data (training set) to the model, allowing it to learn patterns and adjust its               parameters to minimize errors. It helps the model make predictions or classifications.\n",
    "\n",
    " 2. Testing a Dataset: This involves evaluating the trained model on a separate portion of data (test set) to assess its ability to generalize to new,      unseen data. It measures the model's performance using metrics like accuracy or precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b368fae-f2c0-46be-8d80-eead9a5ed31a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d084c68-ae6f-4968-a47e-3f42efe30aa3",
   "metadata": {},
   "source": [
    "Q8) What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbdb95b-f292-41cf-af6e-97f8641d078a",
   "metadata": {},
   "source": [
    "Ans. sklearn.preprocessing is a module in scikit-learn that provides tools for preprocessing data, including scaling, encoding categorical variables, handling missing values, and creating polynomial features, to make the data suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c07c8-fa5f-4fb8-8bc5-9f6b727bef10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ae5664-1dfd-4352-a4b2-b56e4095dbee",
   "metadata": {},
   "source": [
    "Q9) What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a241d7-5c4b-481a-860f-a537da51c3e4",
   "metadata": {},
   "source": [
    "Ans. A test set is a portion of the dataset that is used to evaluate the performance of a machine learning model after it has been trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c7bb6c-2878-4006-b98a-a5b685e0ae46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5402a48-efb0-4b93-9228-ac2a990ae528",
   "metadata": {},
   "source": [
    "Q10) How do we split data for model fitting (training and testing) in Python?\n",
    "     How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b19ae8b-7426-4ad2-afd1-d634eb57d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Python, the most common way to split data for training and testing is using train_test_split from the sklearn.model_selection module.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X: Features, y: Target\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#X: Features (independent variables).\n",
    "#y: Target (dependent variable).\n",
    "#test_size=0.2: Defines the proportion of the data to be used for testing (20% in this case).\n",
    "#random_state=42: Ensures reproducibility by setting a seed for random number generation.\n",
    "#This will split the dataset into 80% for training and 20% for testing by default, but you can adjust the test_size to fit your needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d378aa6-9da5-4f12-9a2d-83c894b6f421",
   "metadata": {},
   "source": [
    "Approach to Solving a Machine Learning Problem:\n",
    "\n",
    "1. Understand the Problem: Define the objective (e.g., classification, regression).\n",
    "\n",
    "2. Data Collection: Gather relevant, clean, and sufficient data.\n",
    "\n",
    "3. Data Preprocessing: Handle missing values, outliers, and encode features.\n",
    "\n",
    "\n",
    "4. Feature Engineering: Select and create relevant features; scale/normalize if needed.\n",
    "\n",
    "\n",
    "5. Model Selection: Choose an appropriate model (e.g., decision tree, regression).\n",
    "\n",
    "\n",
    "6. Model Training: Train the model on the training set and tune hyperparameters.\n",
    "\n",
    "\n",
    "7. Model Evaluation: Evaluate performance using metrics like accuracy, F1-score.\n",
    "\n",
    "\n",
    "8. Model Improvement: Improve by tuning features, model, or hyperparameters.\n",
    "\n",
    "\n",
    "9. Deployment: Deploy the model for real-world use.\n",
    "\n",
    "\n",
    "10. Monitoring: Continuously monitor and update the model as needed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192307eb-0fcc-4f71-bf2d-e2ab7bae20f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15e94236-a724-47fd-977d-2e25883074aa",
   "metadata": {},
   "source": [
    "Q11) Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9862c21-095f-46bf-bfbe-2a8761408977",
   "metadata": {},
   "source": [
    "Ans. Exploratory Data Analysis (EDA) is performed before fitting a model to the data to gain insights and ensure the data is ready for modeling. Here are the key reasons why EDA is important:\n",
    "\n",
    "EDA (Exploratory Data Analysis) is essential before modeling to:\n",
    "\n",
    "1. Understand data structure and relationships.\n",
    "\n",
    "2. Detect missing, invalid data, and outliers.\n",
    "\n",
    "\n",
    "3. Inform feature selection and transformations.\n",
    "\n",
    "4. Check model assumptions.\n",
    "\n",
    "\n",
    "5. Select the appropriate model for the data.\n",
    "\n",
    "\n",
    "6. It ensures the data is clean, well-prepared, and suitable for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b45d3d-8de9-4551-827d-9e27c1d76dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d256274-941f-493d-afe3-d682ad00ab83",
   "metadata": {},
   "source": [
    "Q12) What is correlation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873e02b-5746-4275-a880-8cff00893ff9",
   "metadata": {},
   "source": [
    "Ans. Correlation is a statistical measure that describes the relationship between two variables. It indicates how one variable changes in relation to another and is expressed as a value between -1 and +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5522f7-367d-4d7e-80ef-49e663735bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48e71fd1-839a-4204-b347-33bff2faac11",
   "metadata": {},
   "source": [
    "Q13) What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba8913-560d-41c4-95a9-d7700d2b1f43",
   "metadata": {},
   "source": [
    "Ans. Negative correlation means that as one variable increases, the other variable decreases, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cae586-a62d-447b-9dc8-358afd5b813a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c981dac0-db21-452c-94f9-ade02b105d38",
   "metadata": {},
   "source": [
    "Q14) How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de8c299c-1ad0-4bc7-a590-ebe42b1996bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Variable1  Variable2  Variable3\n",
      "Variable1   1.000000   1.000000  -0.686244\n",
      "Variable2   1.000000   1.000000  -0.686244\n",
      "Variable3  -0.686244  -0.686244   1.000000\n"
     ]
    }
   ],
   "source": [
    "#To find the correlation between variables in Python, you can use statistical libraries such as Pandas, NumPy, or SciPy. Here's how you can do it:\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'Variable1': [1, 2, 3, 4, 5],\n",
    "    'Variable2': [2, 4, 6, 8, 10],\n",
    "    'Variable3': [5, 3, 6, 2, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f3c1f-adbd-4689-a2b5-2c087af8accd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608b19c7-0576-4ddd-81e8-cdb2345b4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using NumPy\n",
    "#The numpy.corrcoef() function calculates the Pearson correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f47803-9b82-4ebd-9592-4f389d2cd052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example variables\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Correlation coefficient\n",
    "correlation = np.corrcoef(x, y)\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1c2d5-90a1-4187-bd7f-992d234f48e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99d13450-c3ba-4dac-8780-cc61c0f30d98",
   "metadata": {},
   "source": [
    "Q15)  What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217b75e6-7ddd-4a61-9e12-d8115f185984",
   "metadata": {},
   "source": [
    "Ans. Causation means that one event directly causes another. In other words, a change in one variable (the cause) directly results in a change in another variable (the effect). Establishing causation requires evidence of a direct cause-and-effect relationship.\n",
    "\n",
    "Difference Between Correlation and Causation\n",
    "\n",
    "1. Correlation: Measures the statistical relationship or association between two variables. Correlation does not imply that one variable causes the other to change. It simply shows that the variables move together in some way (positive, negative, or no relationship).\n",
    "\n",
    "2. Causation: Indicates that one variable's change directly leads to a change in another variable. It implies a cause-and-effect relationship.\n",
    "\n",
    "Example, Suppose we find a positive correlation between ice cream sales and drowning incidents:\n",
    "\n",
    "When ice cream sales increase, drowning incidents also increase.\n",
    "This correlation does not imply causation. It is likely that a third variable, such as hot weather, causes both:\n",
    "\n",
    "Hot weather increases ice cream sales.\n",
    "Hot weather also leads to more people swimming, which can increase the likelihood of drowning.\n",
    "Here, the relationship between ice cream sales and drowning incidents is correlation, not causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab627d-5a6b-4ce4-b887-964ca4a518a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da4d2731-b2c8-412f-bf7a-941538d7f0db",
   "metadata": {},
   "source": [
    "Q16)  What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc104cd5-c14d-4245-b706-22dc9a3b4308",
   "metadata": {},
   "source": [
    "Ans. An optimizer is an algorithm or method used to adjust the weights and biases of a machine learning model during training to minimize the error (loss function). The goal is to improve the model's accuracy and predictive performance by finding the optimal set of parameters.\n",
    "\n",
    "Optimizers are essential in the backpropagation process, where they update the parameters based on the gradients computed from the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd037081-e4e1-41f0-bdc5-48362661d29b",
   "metadata": {},
   "source": [
    "Here’s a quick summary of the different types of optimizers:\n",
    "\n",
    "1. Gradient Descent: Adjusts weights using the negative gradient of the loss function.\n",
    "\n",
    "Variants: Batch, Stochastic (SGD), Mini-Batch.\n",
    "\n",
    "Use: General-purpose, slower convergence.\n",
    "\n",
    "2. Momentum: Speeds up Gradient Descent by adding a fraction of the previous update to the current step.\n",
    "\n",
    "Use: Faster convergence, handles oscillations.\n",
    "\n",
    "3. Adagrad: Adapts learning rate for each parameter based on historical gradients.\n",
    "\n",
    "Use: Sparse data, diminishing learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad3ef5f-efb7-4e7f-8b8c-7a9a5df85f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "# Pseudo code for SGD\n",
    "#weight = weight - learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7db44d9-5cb7-496a-9732-af3b72d9e805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58efb1c1-4270-4751-8668-ba9b575c256c",
   "metadata": {},
   "source": [
    "Q17)  What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95fe14d-b5a7-4291-8ae0-3505e5f32b09",
   "metadata": {},
   "source": [
    "Ans. sklearn.linear_model is a module in scikit-learn, a popular Python library for machine learning, that provides tools for implementing linear models. Linear models are used for predicting a target variable by fitting a linear relationship between the input features and the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7adf14-6f01-48d4-b0d6-6e52c875f78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2351eab4-3372-4b6d-ac2e-299958616a60",
   "metadata": {},
   "source": [
    "Q18) What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae34265-eeba-41cc-8e44-a56459e0135e",
   "metadata": {},
   "source": [
    "Ans. The model.fit() method in scikit-learn is used to train a machine learning model on the given data. It:\n",
    "\n",
    "1. Fits the model to the training data by estimating the parameters (e.g., weights, biases).\n",
    "   \n",
    "2. Learns the relationship between input features (X) and the target variable (y).\n",
    "\n",
    "\n",
    "3. Stores the learned parameters in the model instance for later use (e.g., prediction).\n",
    "\n",
    "Arguments for model.fit():\n",
    "X: Type: 2D array or matrix.\n",
    "Shape: (n_samples, n_features).\n",
    "Purpose: Input features.\n",
    "\n",
    "y: Type: 1D or 2D array.\n",
    "Shape: (n_samples,) or (n_samples, n_targets).\n",
    "Purpose: Target variable(s).\n",
    "\n",
    "Optional:\n",
    "sample_weight: Weights for samples.\n",
    "Model-specific parameters like epochs or callbacks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedbed9c-9352-4600-afa8-a049e03a3849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a43649c5-ecf8-4838-b36c-e062ed094b76",
   "metadata": {},
   "source": [
    "Q19) What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705b50d-b0a4-4010-92cf-dc7d5d2b22fd",
   "metadata": {},
   "source": [
    "Ans. The model.predict() method in scikit-learn is used to make predictions on new, unseen data after a model has been trained using model.fit(). It takes input features and outputs predicted values or labels based on the learned relationship.\n",
    "\n",
    "Arguments for model.predict():\n",
    "\n",
    "X: Type: 2D array or matrix.\n",
    "Shape: (n_samples, n_features).\n",
    "Purpose: Input features for which predictions are made.\n",
    "\n",
    "Output:\n",
    "Regression: Continuous values.\n",
    "Classification: Class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f610e3-3dbc-41fa-91fb-01ac26487b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b91dc73a-ac9d-44c3-a904-b2d9ebaf176c",
   "metadata": {},
   "source": [
    "Q20) What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60628d-061e-4fb1-860e-40d7ec1782a2",
   "metadata": {},
   "source": [
    "Ans. 1. Continuous Variables :\n",
    "\n",
    "Definition: Continuous variables are quantitative variables that can take any value within a given range or interval.\n",
    "\n",
    "Examples:\n",
    "Temperature (e.g., 22.5°C, 25.8°C)\n",
    "Height (e.g., 5.8 feet, 6.1 feet)\n",
    "Weight (e.g., 70.5 kg, 80.3 kg)\n",
    "\n",
    "2. Categorical Variables :\n",
    "   \n",
    "Definition: Categorical variables represent categories or groups that are often qualitative in nature.\n",
    "\n",
    "Examples:\n",
    "Nominal:\n",
    "Colors (e.g., Red, Blue, Green)\n",
    "Gender (e.g., Male, Female, Other)\n",
    "\n",
    "Ordinal:\n",
    "Education level (e.g., High School, Bachelor's, Master's)\n",
    "Rating scale (e.g., Poor, Average, Good, Excellent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2338d9f-448d-4383-b4bd-7b0e9895f4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d31855a2-9c65-4a4c-9602-86a58d9eaf2d",
   "metadata": {},
   "source": [
    "Q21) What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617bac6-0646-4109-941f-0557a584d791",
   "metadata": {},
   "source": [
    "Ans. Feature scaling is the process of transforming the features (input variables) in a dataset to a common scale or range\n",
    "It involves rescaling the features so that they are on a similar scale, making the model training process more efficient and improving performance.\n",
    "\n",
    "Why Feature Scaling is Important in Machine Learning\n",
    "\n",
    "1. Improves Model Performance:\n",
    "\n",
    "Gradient-based algorithms (e.g., Gradient Descent, Logistic Regression, Neural Networks) converge faster when features are scaled because they use the gradient of the loss function to adjust model parameters.\n",
    "\n",
    "Distance-based algorithms (e.g., K-Nearest Neighbors (KNN), Support Vector Machines (SVM)) rely on distances between data points. If one feature has much larger values than another, it can dominate the distance calculations and negatively affect the model's performance.\n",
    "\n",
    "2. Prevents Dominance of Features:\n",
    "\n",
    "Features with larger numerical ranges can overshadow those with smaller ranges, affecting how the model learns.\n",
    "\n",
    "3. Improves Interpretability:\n",
    "\n",
    "Scaling features to the same range can help interpret the importance of each feature in models like linear regression, where coefficients represent feature importance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f1ec1a-977c-42c8-a17e-17077701e107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb49ca41-a36f-4a30-8430-6eb36947604d",
   "metadata": {},
   "source": [
    "Q22) How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e1103bb-223c-4569-9c7f-1a1c6b32b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0. ]\n",
      " [0.5 0.5 0.5]\n",
      " [1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "#Ans. In Python, scaling of features is typically done using the scikit-learn library, which provides various preprocessing functions for scaling. Here’s how you can perform scaling in Python using different methods:\n",
    "\n",
    "#1. Min-Max Scaling (Normalization)\n",
    "#This method rescales the features to a fixed range, typically [0, 1].\n",
    "\n",
    "#Steps:\n",
    "#Use MinMaxScaler from sklearn.preprocessing.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data (features)\n",
    "X = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Output the scaled features\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29373682-9304-4781-86de-c38b125b3f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "481c17dd-d998-479a-89a4-bf0630a3f864",
   "metadata": {},
   "source": [
    "Q23) What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747743d-ab15-48af-8ef0-bb1e24a2b972",
   "metadata": {},
   "source": [
    "Ans. 1. sklearn.preprocessing is a module in scikit-learn that provides a suite of functions and classes to preprocess data before it is used to train machine learning models. These preprocessing techniques are essential for transforming data into a format that can help machine learning algorithms perform optimally.\n",
    "\n",
    "2. sklearn.preprocessing provides tools to scale, normalize, and transform data.\n",
    "\n",
    "3. It helps convert raw data into a form suitable for machine learning algorithms to achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b8cac-af88-43da-89bf-49ecb9557d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d3c8ed-eafd-49af-8228-5911d30b74c4",
   "metadata": {},
   "source": [
    "Q24) How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e0a952c-8d53-41db-b00d-5f53727941cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans. To split data into training and testing sets in Python, you typically use the train_test_split() function from scikit-learn. \n",
    "#This function randomly splits the dataset into two subsets: one for training the model and the other for testing its performance.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]  # Features\n",
    "y = [0, 1, 0, 1, 0]  # Target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a553ccb-20ab-4c18-bf81-88bc27d328f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20691ced-0eec-4d5c-8081-2323f50b8e00",
   "metadata": {},
   "source": [
    "Q25) Explain data encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83207359-49f3-4bdf-aa97-8a0b22ebb9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "#Ans.  1. Data encoding refers to the process of converting categorical data (non-numeric) into a numerical format that machine learning algorithms\n",
    "#can understand.\n",
    "\n",
    "#2. Many machine learning models require numerical input, so encoding categorical features into numeric values is a crucial step in preprocessing.\n",
    "\n",
    "#Types of Data Encoding\n",
    "#Label Encoding\n",
    "\n",
    "#Converts each category into a unique integer.\n",
    "#Best suited for ordinal categorical variables where the categories have a natural order (e.g., \"Low\", \"Medium\", \"High\").\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample categorical data\n",
    "categories = ['Low', 'Medium', 'High', 'Medium', 'Low']\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(categories)\n",
    "\n",
    "print(encoded_data)  # Output: [1, 2, 0, 2, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4b820-9424-4854-a836-a0b45bd40232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
