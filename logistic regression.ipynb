{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQiuSmquKRmThVN8ujko31"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Que :- 1 What is Logistic Regression, and how does it differ from Linear Regression.**\n","\n","Ans:- Logistic Regression and Linear Regression are both statistical methods used for predictive modeling, but they serve different purposes and have different underlying principles. Here's how they compare:\n","\n","---\n","\n","### **Linear Regression:**\n","- **Purpose:** Used for predicting a continuous numerical value.\n","- **Output:** Produces a continuous output (e.g., predicting house prices, temperature).\n","- **Equation Form:** \\( y = \\beta_0 + \\beta_1 x + \\epsilon \\), where:\n","  - \\( y \\) is the dependent variable (continuous)\n","  - \\( x \\) is the independent variable\n","  - \\( \\beta_0 \\) and \\( \\beta_1 \\) are coefficients\n","  - \\( \\epsilon \\) is the error term\n","- **Assumptions:**\n","  - Linearity: Relationship between independent and dependent variables is linear.\n","  - Homoscedasticity: Constant variance of errors.\n","  - Normality of residuals.\n","- **Example Use Case:** Predicting the price of a house based on its size.\n","\n","---\n","\n","### **Logistic Regression:**\n","- **Purpose:** Used for classification tasks, predicting a categorical outcome.\n","- **Output:** Produces a probability (between 0 and 1) that is then mapped to a class label (e.g., Yes/No, True/False).\n","- **Equation Form:** Uses the logistic function (sigmoid function):\n","  \\[\n","  P(Y=1) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x)}}\n","  \\]\n","  - Output is a probability, which is then thresholded (e.g., > 0.5 = 1, otherwise 0).\n","- **Assumptions:**\n","  - Independent variables are linearly related to the log odds of the outcome.\n","  - No multicollinearity among predictors.\n","- **Types of Logistic Regression:**\n","  - **Binary Logistic Regression:** Two possible outcomes (e.g., Spam or Not Spam).\n","  - **Multinomial Logistic Regression:** More than two unordered outcomes.\n","  - **Ordinal Logistic Regression:** Ordered categories.\n","- **Example Use Case:** Predicting whether an email is spam or not.\n","\n","---\n","\n","### **Key Differences:**\n","| Aspect               | Linear Regression                         | Logistic Regression                             |\n","|----------------------|------------------------------------------|--------------------------------------------------|\n","| **Output Type**      | Continuous value (e.g., integers, floats)  | Probability (between 0 and 1)                    |\n","| **Use Case**          | Prediction of numerical values            | Classification (e.g., binary or multiclass)       |\n","| **Linearity**        | Assumes linear relationship                | Assumes linear relationship between predictors and log odds |\n","| **Equation Form**    | \\( y = \\beta_0 + \\beta_1 x \\)              | \\( P(Y=1) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x)}} \\) |\n","| **Activation Function** | None (identity function)                | Sigmoid function for binary classification        |\n","| **Error Metric**     | Mean Squared Error (MSE)                   | Log Loss (Cross-Entropy Loss)                     |\n","\n","---\n","\n","### **Summary:**\n","- **Linear Regression** is best for predicting continuous outcomes.\n","- **Logistic Regression** is ideal for classification problems where the outcome is categorical.\n"],"metadata":{"id":"-txI2LyxiWot"}},{"cell_type":"markdown","source":["## **Que :- 2 What is the mathematical equation of Logistic Regression.**\n","\n","Ans:- The mathematical equation for **Logistic Regression** is based on the **logistic (sigmoid) function**, which maps any real-valued number into a value between 0 and 1. This makes it suitable for probability estimation, particularly for binary classification.\n","\n","---\n","\n","### **1. Basic Form:**\n","The logistic regression model predicts the probability \\( P(Y=1) \\) as follows:\n","\n","\\[\n","P(Y=1) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x)}}\n","\\]\n","\n","Where:\n","- \\( P(Y=1) \\) is the probability of the event occurring (e.g., success, positive class).\n","- \\( \\beta_0 \\) is the intercept (bias term).\n","- \\( \\beta_1 \\) is the coefficient for the predictor variable \\( x \\).\n","- \\( e \\) is the base of the natural logarithm.\n","\n","---\n","\n","### **2. Generalized Form for Multiple Predictors:**\n","For multiple input features (\\( x_1, x_2, ..., x_n \\)), the equation is:\n","\n","\\[\n","P(Y=1) = \\frac{1}{1+e^{-\\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n \\right)}}\n","\\]\n","\n","Or more compactly in vector form:\n","\n","\\[\n","P(Y=1) = \\frac{1}{1+e^{- (X \\cdot \\beta)}}\n","\\]\n","\n","Where:\n","- \\( X = [1, x_1, x_2, ..., x_n] \\) is the vector of input features (including the intercept term as 1).\n","- \\( \\beta = [\\beta_0, \\beta_1, \\beta_2, ..., \\beta_n] \\) is the vector of model parameters.\n","\n","---\n","\n","### **3. Log-Odds (Logit) Form:**\n","Logistic Regression is derived from the linear model using the log-odds transformation:\n","\n","\\[\n","\\text{Logit}(P) = \\log\\left( \\frac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n\n","\\]\n","\n","This shows that logistic regression models the **log-odds** of the probability as a linear combination of the input features.\n","\n","---\n","\n","### **4. Decision Boundary:**\n","In binary classification:\n","- If \\( P(Y=1) > 0.5 \\), predict class 1.\n","- If \\( P(Y=1) \\leq 0.5 \\), predict class 0.\n","\n","This threshold can be adjusted depending on the problem requirements.\n","\n","---\n","\n","### **Summary:**\n","- The core of logistic regression is the **sigmoid function**.\n","- It transforms the linear combination of inputs into a probability.\n","- It predicts the **log-odds** as a linear function of the inputs.\n","- It is widely used for **binary classification** tasks.\n"],"metadata":{"id":"9wRwypj6i1ze"}},{"cell_type":"markdown","source":["## **Que:- 3 Why do we use the Sigmoid function in Logistic Regression.**\n","\n","Ans:- The **Sigmoid function** is used in **Logistic Regression** because it effectively transforms a linear equation into a value between 0 and 1, which can be interpreted as a probability. Here are the key reasons why:\n","\n","---\n","\n","### **1. Probability Mapping:**\n","- The output of the sigmoid function is always between **0 and 1**, which naturally represents a probability.\n","- This makes it ideal for **binary classification**, where the output is the probability of belonging to one class (e.g., Yes/No, Spam/Not Spam).\n","\n","---\n","\n","### **2. Mathematical Form of Sigmoid:**\n","The sigmoid function is defined as:\n","\n","\\[\n","\\sigma(z) = \\frac{1}{1+e^{-z}}\n","\\]\n","\n","Where:\n","- \\( z \\) is the linear combination of the inputs: \\( z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n \\)\n","- \\( e \\) is the base of the natural logarithm.\n","\n","The function smoothly squashes the output:\n","- As \\( z \\) approaches \\( +\\infty \\), \\( \\sigma(z) \\) approaches **1**.\n","- As \\( z \\) approaches \\( -\\infty \\), \\( \\sigma(z) \\) approaches **0**.\n","- When \\( z = 0 \\), \\( \\sigma(z) = 0.5 \\).\n","\n","---\n","\n","### **3. Non-linearity and Decision Boundary:**\n","- The sigmoid function introduces **non-linearity** to the model, allowing logistic regression to model complex decision boundaries.\n","- The decision boundary is at \\( \\sigma(z) = 0.5 \\), corresponding to \\( z = 0 \\), which translates to:\n","  \\[\n","  \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n = 0\n","  \\]\n","  This is a **linear decision boundary** in the feature space.\n","\n","---\n","\n","### **4. Log-Odds Interpretation:**\n","- The sigmoid function naturally fits the **log-odds** interpretation of logistic regression:\n","\\[\n","\\text{Logit}(P) = \\log\\left( \\frac{P}{1-P} \\right) = z\n","\\]\n","This shows that the model is actually fitting a **linear relationship** between the input features and the log-odds of the probability.\n","\n","---\n","\n","### **5. Differentiability and Optimization:**\n","- The sigmoid function is **differentiable**, which is essential for optimization using **Gradient Descent**.\n","- Its derivative is:\n","\\[\n","\\sigma'(z) = \\sigma(z)(1-\\sigma(z))\n","\\]\n","This property makes the calculations in backpropagation efficient, contributing to the fast convergence of the model.\n","\n","---\n","\n","### **Summary of Reasons:**\n","1. **Probability Output:** Maps predictions to the range [0, 1].\n","2. **Non-linearity:** Models complex decision boundaries while maintaining a linear relationship in the log-odds space.\n","3. **Log-Odds Interpretation:** Fits the log-odds of the target variable.\n","4. **Differentiability:** Facilitates efficient optimization using Gradient Descent.\n"],"metadata":{"id":"qZEKG9lLjFyB"}},{"cell_type":"markdown","source":["## **Que :- 4 What is the cost function of Logistic Regression.**\n","\n","Ans:- The **Cost Function** of **Logistic Regression** is known as the **Log Loss** or **Binary Cross-Entropy Loss**. It measures the error between the predicted probabilities and the actual class labels.\n","\n","---\n","\n","### **1. Why Not Mean Squared Error (MSE)?**\n","- Unlike Linear Regression, using **Mean Squared Error (MSE)** is not suitable for Logistic Regression because:\n","  - The sigmoid function is non-linear, leading to a **non-convex loss surface** with multiple local minima if MSE is used.\n","  - This makes optimization difficult and unreliable with Gradient Descent.\n","- **Log Loss** provides a **convex** loss function, ensuring a single global minimum, which is crucial for Gradient Descent to converge effectively.\n","\n","---\n","\n","### **2. Mathematical Form:**\n","The cost function for a single training example is:\n","\n","\\[\n","J(\\beta) = - \\left[ y \\log(h(x)) + (1 - y) \\log(1 - h(x)) \\right]\n","\\]\n","\n","Where:\n","- \\( y \\) = Actual class label (0 or 1).\n","- \\( h(x) \\) = Predicted probability from the sigmoid function:\n","\\[\n","h(x) = \\frac{1}{1+e^{- (\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n)}}\n","\\]\n","- \\( \\beta \\) = Vector of model parameters.\n","\n","---\n","\n","### **3. Intuition Behind the Formula:**\n","- When the actual label is **1**:\n","  - The cost becomes \\( -\\log(h(x)) \\).\n","  - If \\( h(x) \\) is close to 1, the cost is **low**.\n","  - If \\( h(x) \\) is close to 0, the cost becomes **very high**.\n","- When the actual label is **0**:\n","  - The cost becomes \\( -\\log(1-h(x)) \\).\n","  - If \\( h(x) \\) is close to 0, the cost is **low**.\n","  - If \\( h(x) \\) is close to 1, the cost becomes **very high**.\n","\n","This design penalizes confident but wrong predictions heavily, encouraging the model to produce accurate probability estimates.\n","\n","---\n","\n","### **4. Cost Function for the Entire Dataset:**\n","For **m** training examples, the overall cost function is the average of the individual costs:\n","\n","\\[\n","J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\right]\n","\\]\n","\n","Where:\n","- \\( m \\) = Number of training examples.\n","- \\( y^{(i)} \\) = Actual label for the \\( i^{th} \\) example.\n","- \\( x^{(i)} \\) = Feature vector for the \\( i^{th} \\) example.\n","- \\( h(x^{(i)}) \\) = Predicted probability for the \\( i^{th} \\) example.\n","\n","---\n","\n","### **5. Properties of the Cost Function:**\n","- **Convexity:** The cost function is **convex**, meaning it has a single global minimum, which allows **Gradient Descent** to reliably find the optimal parameters.\n","- **Penalizes Confident Errors:** Large penalties for incorrect confident predictions, promoting more accurate probability estimates.\n","\n","---\n","\n","### **6. Gradient Descent Update Rule:**\n","The cost function is minimized using Gradient Descent with the following update rule for each parameter \\( \\beta_j \\):\n","\n","\\[\n","\\beta_j := \\beta_j - \\alpha \\frac{\\partial J(\\beta)}{\\partial \\beta_j}\n","\\]\n","\n","Where:\n","- \\( \\alpha \\) = Learning rate.\n","- \\( \\frac{\\partial J(\\beta)}{\\partial \\beta_j} \\) = Gradient of the cost function w.r.t. \\( \\beta_j \\).\n","\n","---\n","\n","### **Summary:**\n","- Logistic Regression uses the **Log Loss** cost function.\n","- It measures the error between predicted probabilities and actual labels.\n","- It is **convex**, ensuring a global minimum.\n","- It penalizes wrong confident predictions heavily.\n","- It is optimized using **Gradient Descent**.\n"],"metadata":{"id":"Hb9G1F8zjTvU"}},{"cell_type":"markdown","source":["## **Que :- 5 What is Regularization in Logistic Regression? Why is it needed.**\n","\n","Ans:- **Regularization** in **Logistic Regression** is a technique used to prevent **overfitting** by penalizing large coefficients. It adds a complexity penalty to the cost function, ensuring that the model generalizes well to new, unseen data.\n","\n","---\n","\n","### **1. Why is Regularization Needed?**\n","- **Overfitting**: Logistic Regression models can become overly complex if the coefficients (\\( \\beta \\) values) are too large, leading to a model that performs well on training data but poorly on test data.\n","- **Multicollinearity**: If predictors are highly correlated, the model might assign large, unstable weights to these features.\n","- **Generalization**: Regularization helps the model maintain simplicity, leading to better generalization on unseen data.\n","\n","---\n","\n","### **2. How Does Regularization Work?**\n","- Regularization works by adding a penalty term to the **Cost Function** of Logistic Regression, which discourages large coefficients.\n","\n","The modified cost function becomes:\n","\n","\\[\n","J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\right] + \\lambda \\, \\text{Penalty Term}\n","\\]\n","\n","Where:\n","- \\( J(\\beta) \\) = Regularized cost function.\n","- \\( m \\) = Number of training examples.\n","- \\( \\lambda \\) = Regularization parameter (controls the strength of the penalty).\n","- **Penalty Term** depends on the type of regularization (L1 or L2).\n","\n","---\n","\n","### **3. Types of Regularization:**\n","\n","#### **a. L2 Regularization (Ridge Regularization):**\n","- Adds the **squared magnitude** of coefficients as a penalty term:\n","\\[\n","\\text{Penalty Term} = \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\beta_j^2\n","\\]\n","\n","- Cost Function becomes:\n","\\[\n","J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\beta_j^2\n","\\]\n","\n","- **Effect:**\n","  - Encourages smaller but non-zero coefficients.\n","  - Helps in **multicollinearity** situations by shrinking correlated feature weights.\n","  - Does not perform feature selection (all features are retained but with smaller weights).\n","\n","---\n","\n","#### **b. L1 Regularization (Lasso Regularization):**\n","- Adds the **absolute magnitude** of coefficients as a penalty term:\n","\\[\n","\\text{Penalty Term} = \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\beta_j|\n","\\]\n","\n","- Cost Function becomes:\n","\\[\n","J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\right] + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\beta_j|\n","\\]\n","\n","- **Effect:**\n","  - Encourages sparsity by driving some coefficients to exactly zero.\n","  - Performs **feature selection**, as unimportant features get zero weight.\n","  - Useful when dealing with high-dimensional data.\n","\n","---\n","\n","#### **c. Elastic Net Regularization:**\n","- Combines both L1 and L2 penalties:\n","\\[\n","\\text{Penalty Term} = \\frac{\\lambda}{m} \\left[ \\alpha \\sum_{j=1}^{n} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{n} \\beta_j^2 \\right]\n","\\]\n","\n","- **Effect:**\n","  - Combines the feature selection properties of L1 with the stability of L2.\n","  - Controlled by the hyperparameter \\( \\alpha \\), which balances L1 and L2 contributions.\n","\n","---\n","\n","### **4. Choosing the Regularization Parameter (\\( \\lambda \\)):**\n","- \\( \\lambda \\) controls the strength of the penalty:\n","  - **High \\( \\lambda \\)**: Stronger penalty → Smaller coefficients → Underfitting risk.\n","  - **Low \\( \\lambda \\)**: Weaker penalty → Larger coefficients → Overfitting risk.\n","  - \\( \\lambda = 0 \\): No regularization, equivalent to standard Logistic Regression.\n","\n","- Optimal value of \\( \\lambda \\) is typically found using **Cross-Validation**.\n","\n","---\n","\n","### **5. Summary:**\n","- **Purpose:** Regularization prevents overfitting by penalizing large coefficients.\n","- **L2 (Ridge):** Penalizes squared coefficients, keeps all features but shrinks their magnitude.\n","- **L1 (Lasso):** Penalizes absolute coefficients, promotes sparsity and feature selection.\n","- **Elastic Net:** Combines L1 and L2 for a balance of sparsity and stability.\n","- **Tuning \\( \\lambda \\):** Critical for balancing bias and variance, usually optimized via cross-validatioon"],"metadata":{"id":"nIDSY3khjipY"}},{"cell_type":"markdown","source":["## **Que :- 6 Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n","\n","Ans:- **Lasso**, **Ridge**, and **Elastic Net** are regularization techniques used in regression models to prevent overfitting and improve model performance by adding a penalty to the cost function. They differ in how they penalize the coefficients.\n","\n","---\n","\n","## **1. Ridge Regression (L2 Regularization):**\n","- **Penalty Type:**\n","  - Adds an **L2 penalty**, which is the sum of the squared coefficients:\n","\\[\n","\\text{Penalty Term} = \\lambda \\sum_{j=1}^{n} \\beta_j^2\n","\\]\n","\n","- **Cost Function:**\n","\\[\n","J(\\beta) = \\text{MSE} + \\lambda \\sum_{j=1}^{n} \\beta_j^2\n","\\]\n","- **Effect:**\n","  - Shrinks the coefficients towards zero but **does not set them exactly to zero**.\n","  - Keeps all features in the model but reduces their impact.\n","  - Useful for **multicollinearity**, as it stabilizes coefficient estimates.\n","- **Use Case:**\n","  - When you believe all features are relevant but need to be weighted down.\n","  - Works well when many small/medium-sized coefficients are expected.\n","\n","---\n","\n","## **2. Lasso Regression (L1 Regularization):**\n","- **Penalty Type:**\n","  - Adds an **L1 penalty**, which is the sum of the absolute values of the coefficients:\n","\\[\n","\\text{Penalty Term} = \\lambda \\sum_{j=1}^{n} |\\beta_j|\n","\\]\n","\n","- **Cost Function:**\n","\\[\n","J(\\beta) = \\text{MSE} + \\lambda \\sum_{j=1}^{n} |\\beta_j|\n","\\]\n","- **Effect:**\n","  - Can **shrink some coefficients to exactly zero**, effectively performing **feature selection**.\n","  - Leads to **sparse models**, which are easier to interpret.\n","- **Use Case:**\n","  - When you believe only a subset of features is important.\n","  - Useful for **high-dimensional data** where feature selection is necessary.\n","\n","---\n","\n","## **3. Elastic Net Regression:**\n","- **Penalty Type:**\n","  - Combines both **L1 and L2 penalties**:\n","\\[\n","\\text{Penalty Term} = \\lambda \\left[ \\alpha \\sum_{j=1}^{n} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{n} \\beta_j^2 \\right]\n","\\]\n","\n","- **Cost Function:**\n","\\[\n","J(\\beta) = \\text{MSE} + \\lambda \\left[ \\alpha \\sum_{j=1}^{n} |\\beta_j| + (1-\\alpha) \\sum_{j=1}^{n} \\beta_j^2 \\right]\n","\\]\n","  - \\( \\alpha \\) controls the balance between L1 and L2 regularization.\n","- **Effect:**\n","  - Mixes the benefits of **Lasso (sparsity and feature selection)** and **Ridge (shrinkage and stability)**.\n","  - Useful when features are **highly correlated**, as Lasso alone might select one and ignore the others.\n","- **Use Case:**\n","  - When dealing with correlated features and needing both feature selection and model stability.\n","  - When neither Ridge nor Lasso alone works well.\n","\n","---\n","\n","## **4. Summary of Differences:**\n","| Property         | Ridge (L2)                  | Lasso (L1)                     | Elastic Net                        |\n","|------------------|-----------------------------|---------------------------------|-------------------------------------|\n","| **Penalty Type** | \\( \\sum \\beta_j^2 \\)         | \\( \\sum |\\beta_j| \\)            | \\( \\alpha \\sum |\\beta_j| + (1-\\alpha) \\sum \\beta_j^2 \\) |\n","| **Effect on Coefficients** | Shrinks but keeps all nonzero | Some set to exactly zero (feature selection) | Combination of shrinkage and sparsity |\n","| **Feature Selection** | No                        | Yes                             | Yes, but more stable than Lasso      |\n","| **When to Use**  | Multicollinearity, all features relevant | High-dimensional data, sparse solutions | Correlated features, need balance of Ridge and Lasso |\n","\n","---\n","\n","## **5. Key Takeaways:**\n","- **Ridge**: Retains all features but shrinks their impact, useful for correlated features.\n","- **Lasso**: Performs automatic feature selection by setting some coefficients to zero, leading to sparse models.\n","- **Elastic Net**: Balances Ridge and Lasso penalties, ideal for correlated features and when a mix of selection and shrinkage is desired.\n"],"metadata":{"id":"39wGIXRAj4YH"}},{"cell_type":"markdown","source":["## **Que :- 7 When should we use Elastic Net instead of Lasso or Ridge.**\n","\n","Ans:-\n","**Elastic Net** should be used instead of **Lasso** or **Ridge** when dealing with the following scenarios:\n","\n","---\n","\n","### **1. Highly Correlated Features:**\n","- **Problem:**\n","  - **Lasso** tends to select one feature and ignore the rest when predictors are highly correlated.\n","  - This can lead to **unstable models** where small changes in the data can drastically change which feature is selected.\n","- **Elastic Net Solution:**\n","  - Combines **L1 (Lasso)** and **L2 (Ridge)** penalties, encouraging group selection.\n","  - It tends to select or discard **highly correlated features together**, resulting in a more stable model.\n","\n","---\n","\n","### **2. When Neither Lasso nor Ridge Works Well:**\n","- **Lasso Limitations:**\n","  - Performs poorly when:\n","    - There are **more features than observations** (high-dimensional data).\n","    - Features are **highly correlated**.\n","    - True model is not sparse (i.e., many features contribute small effects).\n","- **Ridge Limitations:**\n","  - Retains all features and **does not perform feature selection**.\n","  - Can be less interpretable due to non-zero weights for all predictors.\n","- **Elastic Net Solution:**\n","  - Balances between Lasso's feature selection and Ridge's coefficient shrinkage.\n","  - Ensures **sparsity** while maintaining **stability** and **generalization**.\n","\n","---\n","\n","### **3. Need for Both Feature Selection and Regularization:**\n","- **Lasso** selects features by setting some coefficients to zero, but it might:\n","  - Be **too aggressive** with selection when features are correlated.\n","  - Produce **overly sparse models** if the true underlying model is not sparse.\n","- **Elastic Net Solution:**\n","  - Performs **feature selection** like Lasso but in a more **balanced way**.\n","  - Shrinks correlated features together, leading to better **prediction accuracy**.\n","\n","---\n","\n","### **4. Model Tuning and Flexibility:**\n","- **Elastic Net** introduces a mixing parameter \\( \\alpha \\) that controls the balance:\n","  - \\( \\alpha = 1 \\): Equivalent to **Lasso**.\n","  - \\( \\alpha = 0 \\): Equivalent to **Ridge**.\n","  - \\( 0 < \\alpha < 1 \\): Balances between Lasso and Ridge, offering flexibility.\n","- This flexibility allows fine-tuning of the model, optimizing both:\n","  - **Sparsity** (from Lasso).\n","  - **Stability and multicollinearity handling** (from Ridge).\n","\n","---\n","\n","### **5. Summary:**\n","- Use **Elastic Net** when:\n","  - Features are **highly correlated**, and you want to select groups of features together.\n","  - You need **feature selection** but Lasso is **too aggressive** or unstable.\n","  - You need a balance between **sparsity** and **generalization**.\n","  - Neither Lasso nor Ridge alone provides good performance.\n"],"metadata":{"id":"SpiFHs4ZkLq-"}},{"cell_type":"markdown","source":["## **Que :- 8 What is the impact of the regularization parameter (λ) in Logistic Regression.**\n","\n","Ans:- The **regularization parameter** (\\( \\lambda \\)) in **Logistic Regression** controls the strength of the penalty on the magnitude of the coefficients. It directly impacts the model's complexity and generalization capability.\n","\n","---\n","\n","## **1. Role of \\( \\lambda \\):**\n","- \\( \\lambda \\) determines how much regularization is applied to the cost function:\n","\\[\n","J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})) \\right] + \\lambda \\, \\text{Penalty Term}\n","\\]\n","- **Penalty Term** can be:\n","  - **L2 Regularization (Ridge)**: \\( \\sum \\beta_j^2 \\)\n","  - **L1 Regularization (Lasso)**: \\( \\sum |\\beta_j| \\)\n","  - **Elastic Net**: Combination of L1 and L2\n","\n","---\n","\n","## **2. Impact of \\( \\lambda \\) on Model Behavior:**\n","\n","### **a. \\( \\lambda = 0 \\) (No Regularization):**\n","- The model becomes standard Logistic Regression.\n","- **Effect:**\n","  - No penalty on coefficients → Model is free to fit the training data closely.\n","  - **High risk of overfitting**, especially if the model is complex or the dataset is noisy.\n","  - Coefficients can be large, leading to high variance.\n","\n","---\n","\n","### **b. Small \\( \\lambda \\) (Weak Regularization):**\n","- Slight penalty on coefficients.\n","- **Effect:**\n","  - Model is slightly constrained, but still flexible enough to fit the training data.\n","  - Coefficients are **moderately shrunk**.\n","  - Some risk of overfitting, but generalization might improve slightly compared to no regularization.\n","\n","---\n","\n","### **c. Moderate \\( \\lambda \\):**\n","- Balanced trade-off between bias and variance.\n","- **Effect:**\n","  - Coefficients are **noticeably shrunk**, leading to simpler models.\n","  - **Improved generalization** as the model avoids fitting noise.\n","  - Lower variance and reduced risk of overfitting.\n","\n","---\n","\n","### **d. Large \\( \\lambda \\) (Strong Regularization):**\n","- Strong penalty on coefficients.\n","- **Effect:**\n","  - Coefficients are **significantly shrunk** towards zero.\n","  - Model becomes **simpler and more biased**.\n","  - Can lead to **underfitting**, as the model is too constrained to capture underlying patterns.\n","  - High bias and low variance.\n","\n","---\n","\n","### **e. Extremely Large \\( \\lambda \\):**\n","- Coefficients are shrunk almost to zero.\n","- **Effect:**\n","  - Model becomes too simple, possibly predicting the majority class for all inputs.\n","  - Severe **underfitting** with poor accuracy on both training and test sets.\n","\n","---\n","\n","## **3. Summary of \\( \\lambda \\) Effects:**\n","\n","| \\( \\lambda \\) Value         | Model Complexity    | Coefficients         | Risk of Overfitting | Risk of Underfitting |\n","|-----------------------------|---------------------|----------------------|---------------------|----------------------|\n","| **0 (No Regularization)**   | Very Complex         | Large                | High                | Low                  |\n","| **Small**                   | Slightly Less Complex| Moderately Shrunk     | Moderate             | Low                  |\n","| **Moderate**                | Balanced             | Noticeably Shrunk     | Low                  | Low                  |\n","| **Large**                   | Simple               | Significantly Shrunk  | Very Low             | High                 |\n","| **Extremely Large**         | Too Simple            | Near Zero             | Very Low             | Very High             |\n","\n","---\n","\n","## **4. How to Choose the Right \\( \\lambda \\):**\n","- **Cross-Validation:** Select the optimal \\( \\lambda \\) by evaluating model performance on validation data.\n","- **Grid Search or Random Search**: Tune \\( \\lambda \\) to find the best trade-off between bias and variance.\n","- **Regularization Path Analysis:** Observe how coefficients shrink with increasing \\( \\lambda \\).\n","\n","---\n","\n","## **5. Key Takeaways:**\n","- **Small \\( \\lambda \\)** → Flexible model with risk of overfitting.\n","- **Large \\( \\lambda \\)** → Simpler model with risk of underfitting.\n","- The optimal \\( \\lambda \\) balances **bias and variance** for the best generalization."],"metadata":{"id":"PhdtrFSikcPY"}},{"cell_type":"markdown","source":["## **Que :- 9 What are the key assumptions of Logistic Regression.**\n","\n","Ans:- **Logistic Regression** makes several key assumptions about the data and the relationship between the predictors (independent variables) and the outcome (dependent variable). Understanding these assumptions is important for using Logistic Regression effectively and ensuring the validity of its results.\n","\n","---\n","\n","### **1. Linearity of Log-Odds:**\n","- **Assumption:** There is a **linear relationship** between the independent variables and the **log-odds** of the dependent variable.\n","  - The relationship between the predictors (\\( X_1, X_2, ..., X_n \\)) and the probability of the binary outcome is modeled as a linear combination of the predictors.\n","  - Mathematically:\n","  \\[\n","  \\log\\left( \\frac{p}{1-p} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n\n","  \\]\n","  - This means that the log-odds (logarithm of the odds of the positive class) is a linear function of the input features.\n","\n","---\n","\n","### **2. Independence of Observations:**\n","- **Assumption:** The observations (data points) must be **independent** of each other.\n","  - There should be no autocorrelation or dependence between the data points.\n","  - This assumption is important when the data consists of time series, grouped data, or clustered data, as these situations often lead to dependencies that violate this assumption.\n","\n","---\n","\n","### **3. No or Little Multicollinearity:**\n","- **Assumption:** The independent variables (predictors) should not be **highly correlated** with each other.\n","  - If predictors are highly correlated, it can cause multicollinearity, which leads to unstable coefficient estimates and makes it difficult to interpret the individual effect of each variable.\n","  - Techniques like **Variance Inflation Factor (VIF)** or **Principal Component Analysis (PCA)** can be used to detect and handle multicollinearity.\n","\n","---\n","\n","### **4. Large Sample Size:**\n","- **Assumption:** Logistic Regression requires a sufficiently **large sample size** to provide reliable estimates, especially for estimating the coefficients.\n","  - **Rule of Thumb:** You generally need at least **10 events per predictor variable** in the dataset (i.e., for each independent variable, the number of events of the positive class should be at least 10 times the number of variables).\n","  - Small sample sizes can lead to **overfitting** and **unstable coefficients**.\n","\n","---\n","\n","### **5. No Outliers or Influential Points:**\n","- **Assumption:** Logistic Regression assumes that there are no significant **outliers** or **influential points** in the data that could disproportionately affect the model.\n","  - Outliers can distort the results of the model and lead to biased predictions.\n","  - It's important to analyze the data for any **leverage points** (data points with extreme values) and take appropriate actions (e.g., data transformation, removing outliers).\n","\n","---\n","\n","### **6. Dichotomous Outcome (Binary Dependent Variable):**\n","- **Assumption:** The dependent variable is **binary** (i.e., it takes two values, such as 0 and 1, True and False, or success and failure).\n","  - Logistic Regression is specifically designed for binary classification problems.\n","\n","---\n","\n","### **7. Homoscedasticity of Errors (For Predictions on Continuous Outcome):**\n","- **Assumption:** The variance of the errors (residuals) is assumed to be constant across all levels of the independent variables.\n","  - In practice, this is more relevant for linear regression, but in some cases, the errors in Logistic Regression should exhibit homoscedasticity when predicting probabilities.\n","\n","---\n","\n","### **8. The Probability of Success is Between 0 and 1:**\n","- **Assumption:** The predicted probability from the Logistic Regression model should always lie between 0 and 1.\n","  - This is ensured by the **Sigmoid function**, which outputs a value in the range of [0, 1].\n","\n","---\n","\n","## **Summary of Key Assumptions:**\n","1. **Linearity of Log-Odds**: Linear relationship between predictors and log-odds.\n","2. **Independence of Observations**: No dependencies among data points.\n","3. **No Multicollinearity**: Predictors should not be highly correlated.\n","4. **Large Sample Size**: Sufficient data to estimate coefficients reliably.\n","5. **No Outliers**: No extreme values that could skew results.\n","6. **Binary Outcome**: Dependent variable is binary.\n","7. **Homoscedasticity of Errors**: Constant error variance across levels of independent variables (relevant in some cases).\n","8. **Probability Between 0 and 1**: Logistic Regression guarantees this through the Sigmoid function.\n"],"metadata":{"id":"wYqioE7-lE28"}},{"cell_type":"markdown","source":["## **Que :- 10 What are some alternatives to Logistic Regression for classification tasks.**\n","\n","Ans:- When **Logistic Regression** may not perform well for classification tasks or if you need alternative approaches, there are several other classification algorithms to consider. Here are some popular alternatives to Logistic Regression:\n","\n","---\n","\n","### **1. Decision Trees**\n","- **How it Works:**\n","  - A decision tree splits the data into subsets based on feature values. The process repeats recursively, creating a tree-like structure.\n","  - Each node in the tree represents a decision based on a feature, and each leaf node represents the predicted class.\n","- **Advantages:**\n","  - Simple and interpretable.\n","  - Can handle both numerical and categorical data.\n","  - No need for feature scaling.\n","- **Disadvantages:**\n","  - Prone to **overfitting**, especially with deep trees.\n","  - Sensitive to noisy data and outliers.\n","\n","---\n","\n","### **2. Random Forests**\n","- **How it Works:**\n","  - Random Forest is an ensemble learning method that builds multiple decision trees (typically hundreds or more), each trained on a random subset of the data.\n","  - The final prediction is made by averaging the predictions of individual trees (for regression) or taking the majority vote (for classification).\n","- **Advantages:**\n","  - **Reduces overfitting** by averaging multiple trees.\n","  - Handles missing values and noisy data well.\n","  - Can model complex interactions between features.\n","- **Disadvantages:**\n","  - Can be computationally expensive and slower to train than a single decision tree.\n","  - Less interpretable than individual decision trees.\n","\n","---\n","\n","### **3. Support Vector Machines (SVM)**\n","- **How it Works:**\n","  - SVMs find the hyperplane (or decision boundary) that best separates the classes in the feature space. The hyperplane is chosen to maximize the margin between classes.\n","  - **Kernel trick** can be applied to transform non-linearly separable data into a higher-dimensional space where a linear separator can be found.\n","- **Advantages:**\n","  - Effective for high-dimensional spaces.\n","  - Works well for both linear and non-linear classification problems (with kernels).\n","- **Disadvantages:**\n","  - Can be computationally intensive for large datasets.\n","  - Sensitive to the choice of the kernel and the regularization parameter.\n","\n","---\n","\n","### **4. K-Nearest Neighbors (KNN)**\n","- **How it Works:**\n","  - KNN is a **lazy learning algorithm** that classifies a data point based on the majority class of its **k** nearest neighbors in the feature space.\n","  - The distance metric (e.g., Euclidean distance) is used to identify the nearest neighbors.\n","- **Advantages:**\n","  - Simple and easy to understand.\n","  - No model training phase (instance-based learning).\n","  - Can be used for multi-class classification.\n","- **Disadvantages:**\n","  - Computationally expensive at prediction time (requires calculating distance to all training samples).\n","  - Sensitive to the choice of **k** and the distance metric.\n","  - Performance can degrade with high-dimensional data (curse of dimensionality).\n","\n","---\n","\n","### **5. Naive Bayes Classifier**\n","- **How it Works:**\n","  - Based on **Bayes' Theorem**, Naive Bayes assumes that the features are conditionally independent given the class label.\n","  - It calculates the probabilities of each class and chooses the class with the highest posterior probability.\n","- **Advantages:**\n","  - Simple and fast, especially with large datasets.\n","  - Works well with high-dimensional data (text classification, for example).\n","  - Often performs surprisingly well even when the **independence assumption** is violated.\n","- **Disadvantages:**\n","  - The **naive independence assumption** can limit its accuracy if features are correlated.\n","  - Assumes that each feature contributes equally, which may not always be the case.\n","\n","---\n","\n","### **6. Gradient Boosting Machines (GBM)**\n","- **How it Works:**\n","  - Gradient Boosting is an ensemble technique that builds decision trees **sequentially**, each trying to correct the errors of the previous one.\n","  - The trees are built to minimize a loss function, typically using gradient descent.\n","- **Advantages:**\n","  - High predictive accuracy.\n","  - Can handle different types of data and is robust to outliers.\n","  - Capable of modeling complex non-linear relationships.\n","- **Disadvantages:**\n","  - Sensitive to noise and prone to overfitting if not properly tuned.\n","  - Requires careful tuning of hyperparameters (e.g., learning rate, number of trees).\n","\n","---\n","\n","### **7. Neural Networks (Deep Learning)**\n","- **How it Works:**\n","  - Neural Networks consist of layers of interconnected nodes (neurons), each performing mathematical operations. The network learns weights for each connection by minimizing a loss function through backpropagation.\n","  - **Deep Learning** models (e.g., Deep Neural Networks) use many layers to learn complex patterns in the data.\n","- **Advantages:**\n","  - Extremely powerful for large datasets and complex tasks (e.g., image recognition, natural language processing).\n","  - Can model highly non-linear relationships.\n","- **Disadvantages:**\n","  - Requires a large amount of data and computational resources.\n","  - Training can be slow and requires careful tuning.\n","  - Less interpretable compared to traditional models.\n","\n","---\n","\n","### **8. XGBoost (Extreme Gradient Boosting)**\n","- **How it Works:**\n","  - A highly optimized version of Gradient Boosting that improves on performance, speed, and accuracy.\n","  - It uses techniques like regularization (L1 and L2), tree pruning, and a more efficient loss function.\n","- **Advantages:**\n","  - Extremely fast and efficient.\n","  - High predictive performance, often outperforming other algorithms.\n","  - Handles missing values automatically.\n","- **Disadvantages:**\n","  - Requires careful tuning of parameters.\n","  - Can be computationally expensive with very large datasets.\n","\n","---\n","\n","### **9. LightGBM (Light Gradient Boosting Machine)**\n","- **How it Works:**\n","  - LightGBM is another optimized gradient boosting framework that is designed for efficiency and scalability.\n","  - It uses a **leaf-wise** approach to grow trees, which can lead to better performance for large datasets.\n","- **Advantages:**\n","  - Fast training and efficient memory usage.\n","  - Often performs better on larger datasets compared to XGBoost.\n","- **Disadvantages:**\n","  - Requires careful tuning and is sensitive to hyperparameters.\n","  - May not perform as well for small datasets.\n","\n","---\n","\n","### **Summary of Alternatives:**\n","\n","| Model                         | Strengths                                               | Weaknesses                                       | Use Case                                      |\n","|-------------------------------|---------------------------------------------------------|-------------------------------------------------|-----------------------------------------------|\n","| **Decision Trees**             | Easy to interpret, can handle both numerical and categorical data | Prone to overfitting, sensitive to noise       | Small to medium datasets, interpretability    |\n","| **Random Forest**              | Reduces overfitting, robust to noise, handles complex interactions | Computationally expensive, less interpretable | Medium to large datasets, general-purpose     |\n","| **SVM**                        | Works well in high-dimensional spaces, kernel trick for non-linear data | Computationally expensive, sensitive to kernel choice | Text classification, small to medium datasets |\n","| **KNN**                        | Simple, non-parametric, no training phase               | Computationally expensive, sensitive to distance metric | Small datasets, low-dimensional problems     |\n","| **Naive Bayes**                | Fast, works well with text data, handles high-dimensional data | Independence assumption may limit performance | Text classification, high-dimensional data    |\n","| **GBM (Gradient Boosting)**    | High predictive accuracy, handles non-linear relationships | Can overfit if not tuned, computationally expensive | Complex datasets, competition-winning models  |\n","| **Neural Networks**            | Powerful for large datasets, handles complex relationships | Requires large data and computational power | Image recognition, speech and language tasks |\n","| **XGBoost**                    | Efficient, high accuracy, handles missing values       | Requires tuning, can overfit without regularization | Structured/tabular data, large datasets       |\n","| **LightGBM**                   | Fast, scalable, great for large datasets               | Sensitive to hyperparameters                   | Large datasets, Kaggle competitions          |\n","\n","---\n","\n","### **When to Choose Which Model:**\n","- **Logistic Regression** is great for **binary classification** with **linearly separable data** and when **interpretability** is key.\n","- For **non-linear decision boundaries** and better accuracy, try **Random Forest** or **Gradient Boosting**.\n","- If **high-dimensional data** (e.g., text) is involved, consider **Naive Bayes** or **SVM**.\n","- For very **large datasets** or complex data (e.g., images or sequences), **Neural Networks** or **XGBoost/LightGBM** may be the best choice."],"metadata":{"id":"kfkDRPuQlXaM"}},{"cell_type":"markdown","source":["## **Que :- 11 What are Classification Evaluation Metrics.**\n","\n","Ans:- **Classification Evaluation Metrics** are used to assess the performance of classification models. These metrics help us understand how well the model is predicting the class labels and where it might be making errors. The key evaluation metrics for classification tasks are designed based on the **confusion matrix**.\n","\n","---\n","\n","### **1. Confusion Matrix**\n","\n","A **Confusion Matrix** is a table that shows the number of correct and incorrect predictions for each class in a classification problem.\n","\n","For a binary classification (e.g., predicting positive vs. negative class), the confusion matrix looks like this:\n","\n","|                          | Predicted Positive | Predicted Negative |\n","|--------------------------|--------------------|--------------------|\n","| **Actual Positive**      | True Positive (TP) | False Negative (FN) |\n","| **Actual Negative**      | False Positive (FP)| True Negative (TN)  |\n","\n","Where:\n","- **True Positive (TP)**: Correctly predicted positive cases.\n","- **True Negative (TN)**: Correctly predicted negative cases.\n","- **False Positive (FP)**: Incorrectly predicted positive cases (Type I error).\n","- **False Negative (FN)**: Incorrectly predicted negative cases (Type II error).\n","\n","---\n","\n","### **2. Accuracy**\n","- **Definition**: The proportion of correctly predicted instances (both positive and negative) out of all instances.\n","  \n","  \\[\n","  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n","  \\]\n","  \n","- **When to Use**: Accuracy is useful when the classes are **balanced**, meaning the number of instances in each class is approximately equal.\n","  \n","- **Limitations**: Accuracy can be misleading in imbalanced datasets where the majority class dominates, as predicting the majority class correctly can still yield a high accuracy but poor performance on the minority class.\n","\n","---\n","\n","### **3. Precision (Positive Predictive Value)**\n","- **Definition**: The proportion of positive predictions that are actually correct (how many of the predicted positives are true positives).\n","  \n","  \\[\n","  \\text{Precision} = \\frac{TP}{TP + FP}\n","  \\]\n","  \n","- **When to Use**: Precision is important when the **cost of a false positive** is high. For example, in fraud detection, if you mistakenly classify a legitimate transaction as fraud, it could be costly.\n","\n","---\n","\n","### **4. Recall (Sensitivity or True Positive Rate)**\n","- **Definition**: The proportion of actual positive instances that are correctly predicted (how many of the actual positives are captured by the model).\n","  \n","  \\[\n","  \\text{Recall} = \\frac{TP}{TP + FN}\n","  \\]\n","  \n","- **When to Use**: Recall is important when the **cost of a false negative** is high. For example, in medical diagnoses, failing to detect a disease (false negative) could have serious consequences, so we prioritize high recall.\n","\n","---\n","\n","### **5. F1-Score**\n","- **Definition**: The harmonic mean of **Precision** and **Recall**. The F1-Score combines both metrics into a single value, balancing the trade-off between Precision and Recall.\n","\n","  \\[\n","  \\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n","  \\]\n","  \n","- **When to Use**: F1-Score is useful when you need to balance **Precision** and **Recall**, especially in imbalanced datasets. It is a good overall measure when both false positives and false negatives are of concern.\n","\n","---\n","\n","### **6. Specificity (True Negative Rate)**\n","- **Definition**: The proportion of actual negative instances that are correctly predicted (how many of the actual negatives are correctly identified).\n","  \n","  \\[\n","  \\text{Specificity} = \\frac{TN}{TN + FP}\n","  \\]\n","  \n","- **When to Use**: Specificity is important when the **cost of a false positive** is particularly high and you want to ensure that the model accurately identifies negative instances.\n","\n","---\n","\n","### **7. ROC Curve (Receiver Operating Characteristic Curve)**\n","- **Definition**: A graphical representation that shows the performance of a binary classification model by plotting the **True Positive Rate (Recall)** against the **False Positive Rate**.\n","  \n","  - The **x-axis** represents the **False Positive Rate (FPR)**: \\( \\text{FPR} = \\frac{FP}{FP + TN} \\).\n","  - The **y-axis** represents the **True Positive Rate (Recall)**: \\( \\text{Recall} = \\frac{TP}{TP + FN} \\).\n","\n","- **AUC (Area Under the Curve)**: The area under the ROC curve (AUC) quantifies the overall ability of the model to discriminate between the positive and negative classes. The higher the AUC, the better the model.\n","\n","  - **AUC = 0.5**: The model is performing no better than random guessing.\n","  - **AUC = 1.0**: The model is perfect.\n","\n","- **When to Use**: ROC and AUC are useful when you want to compare different models and assess their performance across various threshold settings.\n","\n","---\n","\n","### **8. Precision-Recall Curve**\n","- **Definition**: A plot of **Precision** against **Recall** for different thresholds.\n","  - Unlike ROC, which is most useful in balanced datasets, the **Precision-Recall curve** is especially valuable for **imbalanced datasets**.\n","\n","- **When to Use**: The Precision-Recall curve is more informative than ROC when the positive class is rare, as it focuses more on the performance with respect to the positive class.\n","\n","---\n","\n","### **9. Logarithmic Loss (Log Loss)**\n","- **Definition**: Log Loss measures the accuracy of a classifier by penalizing incorrect classifications with a logarithmic penalty. It evaluates how well the predicted probabilities match the actual class labels.\n","\n","  \\[\n","  \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i)]\n","  \\]\n","  \n","  Where \\( y_i \\) is the actual label (0 or 1), \\( \\hat{p}_i \\) is the predicted probability for the positive class, and \\( N \\) is the total number of instances.\n","  \n","- **When to Use**: Log Loss is used when you need to evaluate **probability outputs** of classification models rather than just hard predictions. It is useful for **calibrating models** that predict probabilities (e.g., in risk prediction or recommendation systems).\n","\n","---\n","\n","### **10. Matthews Correlation Coefficient (MCC)**\n","- **Definition**: MCC is a more balanced measure than accuracy, especially in imbalanced datasets. It takes into account all four confusion matrix values (TP, TN, FP, FN).\n","\n","  \\[\n","  \\text{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n","  \\]\n","  \n","- **When to Use**: MCC is ideal for **imbalanced datasets**, as it considers both the positive and negative class performance and provides a more balanced metric than accuracy.\n","\n","---\n","\n","### **Summary Table:**\n","\n","| Metric                | Formula                                              | When to Use                                         |\n","|-----------------------|------------------------------------------------------|----------------------------------------------------|\n","| **Accuracy**           | \\( \\frac{TP + TN}{TP + TN + FP + FN} \\)             | Balanced classes, general performance assessment   |\n","| **Precision**          | \\( \\frac{TP}{TP + FP} \\)                            | High cost of false positives (e.g., fraud detection)|\n","| **Recall**             | \\( \\frac{TP}{TP + FN} \\)                            | High cost of false negatives (e.g., disease detection)|\n","| **F1-Score**           | \\( 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} \\) | Balanced trade-off between Precision and Recall    |\n","| **Specificity**        | \\( \\frac{TN}{TN + FP} \\)                            | High cost of false positives                       |\n","| **AUC-ROC**            | -                                                    | General comparison of model discrimination ability |\n","| **Precision-Recall Curve** | -                                                 | Imbalanced datasets, emphasis on positive class    |\n","| **Log Loss**           | \\( -\\frac{1}{N} \\sum [y_i \\log(\\hat{p}_i)] \\)       | Evaluating predicted probabilities, especially in risk models |\n","| **MCC**                | \\( \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}} \\) | Imbalanced datasets, balanced evaluation metric   |\n","\n","---\n","\n","These evaluation metrics help you assess the **trade-offs** between different types of errors (false positives vs. false negatives) and make the appropriate choice depending on the business or practical requirements."],"metadata":{"id":"TBrWUsWFl3MS"}},{"cell_type":"markdown","source":["## **Que :- 12 How does class imbalance affect Logistic Regression.**\n","\n","Ans:- Class imbalance can significantly impact the performance of **Logistic Regression** (and many other machine learning algorithms). When the classes in a classification problem are imbalanced (i.e., one class has much fewer instances than the other), the model might struggle to learn the minority class properly. Here's how class imbalance can affect **Logistic Regression** and some strategies to deal with it:\n","\n","---\n","\n","### **Impact of Class Imbalance on Logistic Regression:**\n","\n","#### 1. **Bias Toward the Majority Class**\n","   - **What Happens**: In an imbalanced dataset, where the majority class (e.g., \"negative\") significantly outnumbers the minority class (e.g., \"positive\"), Logistic Regression can become biased toward predicting the majority class.\n","   - **Example**: If 90% of your data belongs to class 0 (negative) and only 10% belongs to class 1 (positive), the model may predict the majority class (0) most of the time because doing so minimizes the overall error, even though it fails to predict the minority class (1).\n","   - **Consequence**: The model might achieve a high **accuracy** by mostly predicting the majority class but fail to capture the minority class, which could be the more important class in many applications (e.g., fraud detection, disease prediction).\n","\n","---\n","\n","#### 2. **Misleading Accuracy**\n","   - **What Happens**: In imbalanced datasets, **accuracy** can be misleading. Even if the model is predominantly predicting the majority class correctly, the model's overall performance may seem good because accuracy is high, but it’s failing to predict the minority class.\n","   - **Example**: If the dataset is 90% class 0 and 10% class 1, predicting 90% class 0 for all instances would give an accuracy of 90%, but the model would never correctly predict class 1.\n","   - **Consequence**: The model's ability to generalize to both classes is compromised. It could perform well in terms of accuracy but fail in terms of predicting **recall** or **precision** for the minority class.\n","\n","---\n","\n","#### 3. **Poor Recall for the Minority Class**\n","   - **What Happens**: Logistic Regression might fail to capture the minority class entirely, leading to **low recall** (i.e., a high number of **false negatives** for the minority class).\n","   - **Example**: In medical diagnosis (e.g., detecting rare diseases), missing instances of the minority class (e.g., rare diseases) could have serious consequences.\n","   - **Consequence**: The model's failure to identify minority class instances means that it may overlook important predictions, making it unsuitable for applications where **false negatives** are costly (e.g., cancer detection).\n","\n","---\n","\n","#### 4. **Distorted Decision Boundary**\n","   - **What Happens**: In Logistic Regression, the decision boundary is influenced by the class distribution. When one class dominates the other, the decision boundary will be skewed toward the majority class.\n","   - **Example**: The model will place the decision threshold in such a way that the minority class is poorly represented, resulting in poor generalization for the minority class.\n","   - **Consequence**: The model may fail to distinguish between the two classes, especially when the minority class is crucial for decision-making (e.g., fraud detection).\n","\n","---\n","\n","### **Strategies to Address Class Imbalance in Logistic Regression:**\n","\n","#### 1. **Resampling Techniques**\n","   - **Oversampling the Minority Class**: You can increase the number of instances in the minority class by **duplicating** or generating synthetic data (e.g., using **SMOTE** — Synthetic Minority Over-sampling Technique).\n","     - **Pro**: This balances the dataset and provides the model more examples of the minority class.\n","     - **Con**: Oversampling might lead to **overfitting** since the model may learn too much from the duplicated or synthetic examples.\n","   \n","   - **Undersampling the Majority Class**: You can reduce the number of instances in the majority class to balance the class distribution.\n","     - **Pro**: Helps avoid overfitting and speeds up training.\n","     - **Con**: You lose information from the majority class, which could degrade the model’s performance.\n","\n","#### 2. **Class Weights Adjustment**\n","   - **What Happens**: Logistic Regression allows you to assign **weights** to the classes during training. By assigning a higher weight to the minority class, you penalize the model more for misclassifying the minority class.\n","   - **How It Works**: In **sklearn's LogisticRegression**, you can use the `class_weight='balanced'` argument to automatically adjust the weights inversely proportional to class frequencies.\n","   - **Formula**: The weight for each class is calculated as:\n","     \\[\n","     \\text{Class Weight} = \\frac{\\text{Total Instances}}{\\text{Number of Classes} \\times \\text{Number of Instances in Class}}\n","     \\]\n","   - **Pro**: This helps the model pay more attention to the minority class and improves performance in imbalanced datasets.\n","   - **Con**: It can make the model more sensitive to noise in the minority class, especially if the data quality is poor.\n","\n","#### 3. **Anomaly Detection**\n","   - **What Happens**: If the minority class represents rare events (e.g., fraud, rare diseases), **anomaly detection** methods can be used instead of traditional classification methods.\n","   - **Pro**: Focuses specifically on detecting outliers (rare events) without the need for balancing the dataset.\n","   - **Con**: Might not work well if the minority class is not truly anomalous and has an underlying distribution that needs to be learned.\n","\n","#### 4. **Using Alternative Metrics**\n","   - **What Happens**: When dealing with imbalanced datasets, accuracy alone isn’t enough to evaluate the model's performance. You should focus on metrics like:\n","     - **Precision**: How many of the predicted positive instances are actually positive.\n","     - **Recall**: How many of the actual positive instances were correctly predicted.\n","     - **F1-Score**: Harmonic mean of Precision and Recall.\n","     - **AUC-ROC**: Measures the trade-off between recall and false positive rate across various thresholds.\n","     - **Precision-Recall Curve**: Especially useful for imbalanced datasets.\n","\n","#### 5. **Ensemble Methods**\n","   - **What Happens**: Ensemble models like **Random Forests** or **Gradient Boosting** (e.g., **XGBoost** or **LightGBM**) can be more robust to class imbalance because they combine predictions from multiple models, making them more accurate on both classes.\n","   - **Pro**: Ensemble methods can help mitigate the bias introduced by Logistic Regression.\n","   - **Con**: They can be more complex and harder to interpret.\n","\n","#### 6. **Threshold Adjustment**\n","   - **What Happens**: In Logistic Regression, the default decision threshold for predicting class labels is typically 0.5 (i.e., if the predicted probability is greater than 0.5, classify as positive, otherwise negative).\n","   - **Strategy**: You can adjust the decision threshold to be more favorable for the minority class (e.g., lower the threshold to classify more instances as positive).\n","   - **Pro**: This allows you to balance the predictions for both classes more effectively.\n","   - **Con**: It can lead to more false positives or negatives, depending on how the threshold is adjusted.\n","\n","---\n","\n","### **Summary of Effects and Solutions:**\n","\n","| **Issue**                           | **Effect**                                                        | **Solution**                                          |\n","|-------------------------------------|------------------------------------------------------------------|------------------------------------------------------|\n","| **Bias Toward Majority Class**     | Model underperforms on minority class.                         | Resampling (oversampling/undersampling), class weights. |\n","| **Misleading Accuracy**            | High accuracy but poor performance on minority class.           | Use precision, recall, F1-score, and AUC-ROC for evaluation. |\n","| **Poor Recall for Minority Class** | Misses many instances of the minority class.                    | Resampling, class weights, adjusting thresholds.      |\n","| **Distorted Decision Boundary**    | Majority class dominates the decision boundary.                 | Resampling, class weights, anomaly detection.         |\n","\n","---\n","\n","### **Conclusion:**\n","Class imbalance poses a significant challenge to Logistic Regression, especially when the minority class is of high importance. However, there are several methods to address this issue, including resampling, adjusting class weights, using alternative evaluation metrics, and considering more advanced models (e.g., ensemble methods). By addressing the imbalance, you can improve the model’s ability to make accurate predictions for both classes."],"metadata":{"id":"jZu9NWTQmSYM"}},{"cell_type":"markdown","source":["## **Que ;- 13 What is Hyperparameter Tuning in Logistic Regression.**\n","\n","Ans:- **Hyperparameter tuning** in **Logistic Regression** refers to the process of selecting the best combination of hyperparameters (settings that govern the model training) to improve the model's performance. Unlike **model parameters** (which are learned during training, such as the coefficients), **hyperparameters** are set before training and can significantly impact the model's ability to generalize well on unseen data.\n","\n","In **Logistic Regression**, common hyperparameters include regularization strength, solver choice, and the type of regularization (L1 or L2). Here's a breakdown of important hyperparameters to tune and the methods you can use to find the best values:\n","\n","---\n","\n","### **Key Hyperparameters in Logistic Regression:**\n","\n","1. **Regularization Strength (`C`)**\n","   - **Definition**: The `C` parameter controls the strength of regularization in the logistic regression model. Regularization helps prevent overfitting by penalizing large coefficients.\n","   - **Effect**:\n","     - A **small value of `C`** (e.g., 0.001) applies **stronger regularization**, which makes the model simpler but could lead to underfitting.\n","     - A **large value of `C`** (e.g., 1000) applies **weaker regularization**, allowing the model to fit the training data more closely, but it may lead to overfitting.\n","   - **How to Tune**: Choose a value of `C` that balances the bias-variance trade-off.\n","\n","2. **Regularization Type (`penalty`)**\n","   - **Definition**: Determines the type of regularization applied to the model:\n","     - `l1` (Lasso) for **L1 regularization**: Encourages sparsity (many coefficients become zero).\n","     - `l2` (Ridge) for **L2 regularization**: Penalizes large coefficients but does not enforce sparsity.\n","   - **Effect**:\n","     - **L1 regularization** is useful when you suspect that only a few features are important, leading to feature selection.\n","     - **L2 regularization** is better when you believe all features should contribute but in a constrained manner.\n","   - **How to Tune**: Try both `l1` and `l2` regularization and choose the one that yields better model performance.\n","\n","3. **Solver (`solver`)**\n","   - **Definition**: The algorithm used to optimize the cost function. Common solvers include:\n","     - `liblinear`: Good for small datasets and L1 regularization.\n","     - `newton-cg`, `lbfgs`, `sag`, and `saga`: Suitable for larger datasets and L2 regularization.\n","   - **Effect**: The choice of solver can impact both speed and performance. For example, `liblinear` is slower for larger datasets, while `saga` can handle both L1 and L2 regularization efficiently.\n","   - **How to Tune**: Try different solvers to see which one works best for your data, especially if the dataset is large.\n","\n","4. **Max Iterations (`max_iter`)**\n","   - **Definition**: The maximum number of iterations the solver is allowed to run to converge to a solution.\n","   - **Effect**: Too few iterations might result in underfitting, as the algorithm doesn’t have enough time to converge. Too many iterations could lead to unnecessary computation.\n","   - **How to Tune**: Start with a reasonable default (e.g., 100) and increase it if the model doesn't converge.\n","\n","5. **Tolerance (`tol`)**\n","   - **Definition**: The stopping criterion for the optimization algorithm. If the change in the cost function is smaller than `tol`, the algorithm stops.\n","   - **Effect**: A smaller tolerance ensures more precise optimization but could increase computation time.\n","   - **How to Tune**: Lower the `tol` value if you want more precise results, but this will also increase the training time.\n","\n","---\n","\n","### **Hyperparameter Tuning Methods:**\n","\n","1. **Grid Search Cross-Validation**\n","   - **Definition**: A method where you exhaustively search over a manually specified grid of hyperparameters. Grid search performs cross-validation for each combination of hyperparameters to determine the best set.\n","   - **How to Use**: In Python's **scikit-learn**, `GridSearchCV` can be used to perform grid search cross-validation.\n","   - **Example**: Tuning `C`, `penalty`, and `solver` hyperparameters.\n","   ```python\n","   from sklearn.model_selection import GridSearchCV\n","   from sklearn.linear_model import LogisticRegression\n","   \n","   # Define the model\n","   model = LogisticRegression()\n","   \n","   # Define hyperparameter grid\n","   param_grid = {\n","       'C': [0.001, 0.01, 0.1, 1, 10],\n","       'penalty': ['l1', 'l2'],\n","       'solver': ['liblinear', 'saga'],\n","   }\n","   \n","   # Grid search with cross-validation\n","   grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n","   grid_search.fit(X_train, y_train)\n","   \n","   # Best hyperparameters\n","   print(grid_search.best_params_)\n","   ```\n","\n","2. **Random Search Cross-Validation**\n","   - **Definition**: Instead of searching through every combination like grid search, random search selects a random combination of hyperparameters and performs cross-validation. This can be more efficient when there are many hyperparameters.\n","   - **How to Use**: In **scikit-learn**, `RandomizedSearchCV` can be used for random search cross-validation.\n","   - **Example**: Tuning `C` and `penalty` with a random search.\n","   ```python\n","   from sklearn.model_selection import RandomizedSearchCV\n","   from sklearn.linear_model import LogisticRegression\n","   from scipy.stats import uniform\n","   \n","   # Define the model\n","   model = LogisticRegression()\n","   \n","   # Define the hyperparameter distribution\n","   param_dist = {\n","       'C': uniform(0.001, 1000),  # Randomly sampling C in this range\n","       'penalty': ['l1', 'l2'],\n","       'solver': ['liblinear', 'saga'],\n","   }\n","   \n","   # Random search with cross-validation\n","   random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, cv=5, n_iter=10)\n","   random_search.fit(X_train, y_train)\n","   \n","   # Best hyperparameters\n","   print(random_search.best_params_)\n","   ```\n","\n","3. **Bayesian Optimization**\n","   - **Definition**: Bayesian optimization uses a probabilistic model to predict the performance of different hyperparameter combinations and chooses the most promising ones to evaluate next.\n","   - **Pro**: It is more efficient than grid or random search, especially for expensive models.\n","   - **Con**: It is more complex and requires additional libraries like **`hyperopt`** or **`optuna`**.\n","\n","4. **Manual Tuning (Heuristic)**\n","   - **Definition**: A simple approach where you manually try different hyperparameter values based on your knowledge of the dataset and problem.\n","   - **When to Use**: If you have domain knowledge or experience with similar datasets, this can be a faster approach. However, it's not as exhaustive or systematic as the other methods.\n","\n","---\n","\n","### **Best Practices for Hyperparameter Tuning:**\n","\n","1. **Start with Default Values**: Begin with default hyperparameter values and gradually experiment with tuning.\n","   \n","2. **Use Cross-Validation**: Always evaluate model performance with cross-validation (e.g., 5-fold or 10-fold) to ensure that your tuning doesn't overfit to the training set.\n","\n","3. **Tune Regularization First**: Since regularization is critical in controlling overfitting, start by tuning the `C` parameter (the regularization strength).\n","\n","4. **Use a Validation Set**: After performing hyperparameter tuning, check the model performance on a separate validation set to evaluate its generalization ability.\n","\n","5. **Consider Computational Resources**: Some hyperparameter search methods (like grid search) can be computationally expensive. If your dataset is large or the model takes a long time to train, consider using random search or Bayesian optimization to reduce the search space.\n","\n","---\n","\n","### **Summary:**\n","Hyperparameter tuning in **Logistic Regression** involves selecting the best set of hyperparameters (e.g., regularization strength, regularization type, solver, max iterations) to optimize model performance. Common techniques include **Grid Search**, **Random Search**, and **Bayesian Optimization**. Regularization parameters are typically the first things to tune, followed by solvers and iteration settings. Cross-validation is key to ensuring that the tuned model generalizes well on unseen data."],"metadata":{"id":"d0m0s5s9msGh"}},{"cell_type":"markdown","source":["## **Que :- 14 What are different solvers in Logistic Regression? Which one should be used.**\n","\n","Ans:- In **Logistic Regression**, the **solver** is the algorithm used to find the optimal model parameters by minimizing the **cost function** (usually, the **log-loss** or **binary cross-entropy**). Different solvers use different optimization techniques to perform this task, and the choice of solver can impact both the speed and the accuracy of the solution.\n","\n","Here are the main solvers available in **scikit-learn's LogisticRegression**:\n","\n","### **1. 'liblinear'**\n","   - **Type**: **Optimization** using the **liblinear** library, based on **Coordinate Descent**.\n","   - **Regularization**: Works well with both **L1 (Lasso)** and **L2 (Ridge)** regularization.\n","   - **Suitable For**: Small datasets and when using **L1 regularization** (Lasso), as it is efficient for problems with sparse solutions (many coefficients being zero).\n","   - **Advantages**:\n","     - Good for smaller datasets (less than a few thousand samples).\n","     - Suitable for **L1 regularization**.\n","   - **Disadvantages**:\n","     - Slower for larger datasets compared to some other solvers.\n","     - May not scale well for large datasets.\n","   \n","### **2. 'newton-cg'**\n","   - **Type**: **Newton's Conjugate Gradient** method.\n","   - **Regularization**: Works only with **L2 regularization** (Ridge).\n","   - **Suitable For**: Larger datasets, especially when you are using **L2 regularization**.\n","   - **Advantages**:\n","     - Faster than **'liblinear'** on larger datasets.\n","     - Efficient for **L2 regularization**.\n","     - Converges quickly in some cases, especially when the dataset has a lot of features.\n","   - **Disadvantages**:\n","     - Does not support **L1 regularization** (Lasso).\n","     - Can be slower when there are many **features** (high-dimensional data).\n","\n","### **3. 'lbfgs'**\n","   - **Type**: **Limited-memory Broyden–Fletcher–Goldfarb–Shanno (BFGS)**, which is a quasi-Newton method.\n","   - **Regularization**: Works only with **L2 regularization** (Ridge).\n","   - **Suitable For**: Large datasets, especially when the dataset has many features and you need **L2 regularization**.\n","   - **Advantages**:\n","     - Good for large datasets and high-dimensional data.\n","     - Efficient and widely used.\n","   - **Disadvantages**:\n","     - Does not support **L1 regularization** (Lasso).\n","     - May need more memory and time for very large datasets.\n","\n","### **4. 'sag' (Stochastic Average Gradient)**\n","   - **Type**: **Stochastic gradient descent** method.\n","   - **Regularization**: Works with **L2 regularization** (Ridge).\n","   - **Suitable For**: Very large datasets.\n","   - **Advantages**:\n","     - Scales well with **very large datasets**.\n","     - Faster for large datasets compared to **'newton-cg'** and **'lbfgs'**.\n","     - Can be efficient for problems with a **large number of features**.\n","   - **Disadvantages**:\n","     - Can be less accurate compared to other solvers for small datasets or when there are very few features.\n","     - Slower when there is a lot of noise in the data.\n","\n","### **5. 'saga'**\n","   - **Type**: **Stochastic Average Gradient** (an improved version of **sag**).\n","   - **Regularization**: Works with both **L1 (Lasso)** and **L2 (Ridge)** regularization.\n","   - **Suitable For**: Large datasets and when you need **L1 regularization** (Lasso).\n","   - **Advantages**:\n","     - Efficient and faster for large datasets.\n","     - Supports both **L1** and **L2 regularization**.\n","     - Suitable for **high-dimensional** datasets and problems where many coefficients are sparse (L1).\n","   - **Disadvantages**:\n","     - Similar to **sag**, it can be less accurate for smaller datasets or when the data is noisy.\n","\n","---\n","\n","### **Summary of Solvers and Recommendations:**\n","\n","| **Solver**      | **Regularization** | **Suitable For**                     | **Pros**                                          | **Cons**                                              |\n","|-----------------|--------------------|--------------------------------------|--------------------------------------------------|------------------------------------------------------|\n","| **'liblinear'**  | L1 and L2          | Small datasets, L1 regularization    | Works well with sparse data, suitable for small datasets | Slower for large datasets                            |\n","| **'newton-cg'**  | L2                 | Large datasets, L2 regularization    | Fast convergence, works well with high-dimensional data | Doesn't support L1 regularization                   |\n","| **'lbfgs'**      | L2                 | Large datasets, high-dimensional data | Efficient for large datasets, widely used       | Doesn't support L1 regularization                   |\n","| **'sag'**        | L2                 | Very large datasets                  | Scales well for large datasets                   | Less accurate for smaller datasets and noisy data    |\n","| **'saga'**       | L1 and L2          | Large datasets, L1 regularization    | Efficient, supports both L1 and L2 regularization | Less accurate for small datasets                    |\n","\n","---\n","\n","### **Which Solver to Use?**\n","\n","- **For small datasets with L1 regularization**: Use `'liblinear'`. It performs well with small data and supports L1 regularization.\n","  \n","- **For large datasets with L2 regularization**: Use `'lbfgs'` or `'newton-cg'`. These solvers are efficient for large datasets, but `'lbfgs'` is often preferred due to its good balance between speed and accuracy.\n","\n","- **For large datasets with both L1 and L2 regularization**: Use `'saga'`. It supports both types of regularization and is designed for large-scale problems.\n","\n","- **For very large datasets with L2 regularization**: Use `'sag'`. It scales well for very large datasets but is generally less accurate than `'lbfgs'` or `'saga'` for smaller datasets.\n","\n","---\n","\n","### **In Summary**:\n","- Use **'liblinear'** for small datasets and L1 regularization.\n","- Use **'newton-cg'**, **'lbfgs'** for larger datasets with L2 regularization.\n","- Use **'saga'** for large datasets and when both **L1** and **L2 regularization** are needed."],"metadata":{"id":"ZgKEaLF7nWnu"}},{"cell_type":"markdown","source":["## **Que :- 15 How is Logistic Regression extended for multiclass classification.**\n","\n","Ans:- In **Logistic Regression**, the standard approach works for binary classification problems (i.e., when the target variable has two classes). However, for **multiclass classification** (i.e., when the target variable has more than two classes), **Logistic Regression** can be extended using two primary techniques: **One-vs-Rest (OvR)** and **Multinomial Logistic Regression**. Here's how each works:\n","\n","---\n","\n","### **1. One-vs-Rest (OvR) or One-vs-All (OvA)**\n","\n","In **One-vs-Rest** (also known as **One-vs-All**), the idea is to train multiple binary classifiers, each distinguishing one class from all the others.\n","\n","#### **How it works:**\n","- For each class, a separate binary classifier is trained. The classifier will predict whether the data point belongs to that class or not (i.e., belonging to the \"rest\" of the classes).\n","- This results in one classifier for each class, so if you have **K classes**, you will train **K classifiers**.\n","- During prediction, each classifier outputs a probability for its class, and the class with the highest probability is chosen as the predicted class.\n","\n","#### **Example:**\n","If you have three classes (`A`, `B`, and `C`), you will train three classifiers:\n","- Classifier 1: Class A vs. Class B or C\n","- Classifier 2: Class B vs. Class A or C\n","- Classifier 3: Class C vs. Class A or B\n","\n","The model predicts the class with the highest score from all classifiers.\n","\n","#### **Advantages:**\n","- Simple and easy to implement.\n","- Works well for many classes.\n","\n","#### **Disadvantages:**\n","- The classifiers are independent, and there may be competition between them (i.e., some classes may overlap).\n","- Doesn't take into account the possible relationship between classes.\n","- Computationally expensive for a large number of classes.\n","\n","---\n","\n","### **2. Multinomial Logistic Regression (Softmax Regression)**\n","\n","**Multinomial Logistic Regression**, also known as **Softmax Regression**, generalizes logistic regression for multiclass problems by using the **softmax function** instead of the sigmoid function.\n","\n","#### **How it works:**\n","- Instead of having multiple binary classifiers, **one model is trained** with multiple outputs (one for each class).\n","- The softmax function is applied to the linear scores of each class, which converts them into probabilities.\n","  \n","#### **Softmax Function:**\n","The **softmax function** is used to compute the probability of each class:\n","\\[\n","P(y = k | x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}\n","\\]\n","Where:\n","- \\(P(y = k | x)\\) is the probability of class \\(k\\) given input \\(x\\),\n","- \\(\\theta_k\\) are the parameters (weights) for class \\(k\\),\n","- \\(x\\) is the feature vector,\n","- \\(K\\) is the total number of classes.\n","\n","The denominator is a sum over the exponentiated values for all classes, ensuring that the probabilities sum up to 1.\n","\n","#### **Advantages:**\n","- More efficient than training multiple binary classifiers.\n","- Models the relationship between all classes simultaneously.\n","- The output is a probability distribution over all classes, so the class with the highest probability is naturally chosen.\n","\n","#### **Disadvantages:**\n","- Requires more complex training since it is a multi-output model.\n","- Can be less interpretable than One-vs-Rest in some cases.\n","\n","---\n","\n","### **Which to Use?**\n","\n","- **One-vs-Rest (OvR)**: This method is often preferred when you have a large number of classes and you want a simple and efficient model. It works well in practice and is supported by many machine learning libraries, including **scikit-learn**. However, it can be less accurate in certain situations due to the independence assumption between classifiers.\n","  \n","- **Multinomial Logistic Regression (Softmax Regression)**: This method is typically used when there is a need to model the interactions between classes, and it is particularly useful for problems where the classes have a natural ordering (e.g., multi-class classification with ordinal labels). It is also often preferred when the model's goal is to produce a true probability distribution over the classes.\n","\n","---\n","\n","### **In scikit-learn:**\n","\n","When you use **LogisticRegression** in **scikit-learn**, you can specify the `multi_class` parameter to choose between OvR or multinomial logistic regression. The `solver` you choose will also determine the exact approach used:\n","\n","1. **One-vs-Rest (OvR)**:\n","   - This is the default setting for **binary logistic regression** and is also used for multiclass when using solvers like **'liblinear'**.\n","   - Set `multi_class='ovr'`.\n","\n","2. **Multinomial Logistic Regression**:\n","   - You can use multinomial logistic regression by setting `multi_class='multinomial'`.\n","   - The solvers that support this are **'newton-cg'**, **'lbfgs'**, and **'saga'**.\n","   - Example:\n","   ```python\n","   from sklearn.linear_model import LogisticRegression\n","\n","   # Define the model with multinomial logistic regression\n","   model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n","   model.fit(X_train, y_train)\n","   ```\n","\n","---\n","\n","### **Summary of Key Differences:**\n","\n","| **Feature**                   | **One-vs-Rest (OvR)**                           | **Multinomial Logistic Regression**       |\n","|-------------------------------|-------------------------------------------------|--------------------------------------------|\n","| **Method**                     | Multiple binary classifiers                     | One model with multiple outputs (Softmax)  |\n","| **Output**                     | Binary predictions for each class               | Single prediction (class with highest probability) |\n","| **Performance**                | Can be computationally expensive with many classes | More efficient with fewer models for many classes |\n","| **Model Relationships**        | No interaction between classes                  | Models the relationship between all classes |\n","| **Support for L1 Regularization** | Supported                                    | Supported                                 |\n","| **Best For**                   | Simple classification problems with many classes | Problems where class relationships matter |"],"metadata":{"id":"Gvkade7goT5Q"}},{"cell_type":"markdown","source":["## **Que :- 16 What are the advantages and disadvantages of Logistic Regression.**\n","\n","Ans:- **Logistic Regression** is one of the most commonly used algorithms for classification tasks, but like all models, it has its pros and cons. Here's a breakdown of its **advantages** and **disadvantages**:\n","\n","---\n","\n","### **Advantages of Logistic Regression:**\n","\n","1. **Simplicity and Interpretability**:\n","   - **Easy to understand and implement**: Logistic regression is one of the simplest algorithms in machine learning. Its probabilistic framework makes it easy to understand and interpret, especially when compared to more complex models like deep learning or ensemble methods.\n","   - **Coefficients Interpretation**: The coefficients in logistic regression can be directly interpreted as the **log-odds** of a class. This makes it easier to explain how individual features influence the classification outcome.\n","\n","2. **Efficient for Linearly Separable Data**:\n","   - If the data is **linearly separable** (i.e., there is a clear boundary that separates classes), logistic regression can give excellent results with minimal computational effort.\n","   - It's particularly useful when your dataset follows a linear decision boundary, making it fast and efficient for such tasks.\n","\n","3. **Less Prone to Overfitting (with Regularization)**:\n","   - Logistic regression is relatively simple, so it’s less likely to overfit, especially if regularization techniques such as **L1 (Lasso)** or **L2 (Ridge)** are used to prevent overfitting.\n","   - Regularization can help improve generalization performance on unseen data.\n","\n","4. **Probability Estimates**:\n","   - Logistic regression provides **probabilities** for each class, making it easy to assess the confidence of the predictions.\n","   - This is useful in cases where it's important to understand not just the predicted class but the certainty of the prediction.\n","\n","5. **Works Well with Binary and Multiclass Problems**:\n","   - It is naturally suited for **binary classification** but can also be extended to **multiclass classification** using techniques like **One-vs-Rest (OvR)** or **Multinomial Logistic Regression**.\n","\n","6. **Scalability**:\n","   - Logistic regression can scale well to large datasets, especially when used with efficient solvers such as **‘liblinear’** or **‘saga’**.\n","   - It performs well in problems with a large number of features, especially with proper regularization.\n","\n","---\n","\n","### **Disadvantages of Logistic Regression:**\n","\n","1. **Limited to Linear Decision Boundaries**:\n","   - Logistic regression assumes a **linear relationship** between the input features and the log-odds of the outcome. If the true relationship is **non-linear**, logistic regression may fail to capture it.\n","   - For **complex decision boundaries**, non-linear models like **Support Vector Machines (SVM)**, **Random Forests**, or **Neural Networks** may perform better.\n","\n","2. **Sensitive to Feature Scaling**:\n","   - Logistic regression is sensitive to the scale of the input features, so **feature scaling** (such as standardization or normalization) is often required.\n","   - Without proper scaling, the optimization process might be slower and less effective.\n","\n","3. **Not Ideal for Large Number of Categorical Features**:\n","   - Logistic regression doesn’t handle **high cardinality categorical variables** (i.e., categorical variables with many levels) well without proper preprocessing.\n","   - You need to carefully encode categorical features (e.g., using one-hot encoding or regularization) to avoid performance issues.\n","\n","4. **Assumes No Multicollinearity**:\n","   - Logistic regression assumes that the input features are **independent** of each other. In the presence of **multicollinearity** (high correlation between features), it can cause **unstable estimates** of the coefficients and inflate standard errors.\n","   - Techniques like **Principal Component Analysis (PCA)** or regularization (L1 or L2) can help mitigate this.\n","\n","5. **Over-simplified for Complex Problems**:\n","   - While logistic regression is a simple model, it may be too simplistic for complex datasets where the relationships between features and the target variable are non-linear or involve intricate interactions.\n","   - Models like **Random Forests**, **Gradient Boosting Machines (GBM)**, or **Deep Learning** may be better suited for such cases.\n","\n","6. **Binary Classifications Only for Logistic Regression**:\n","   - Standard logistic regression is primarily designed for **binary classification**. While it can be extended to **multiclass classification**, this adds complexity and requires specialized approaches like **One-vs-Rest (OvR)** or **Multinomial Logistic Regression**.\n","   \n","---\n","\n","### **Summary Table:**\n","\n","| **Advantages**                                        | **Disadvantages**                                        |\n","|-------------------------------------------------------|---------------------------------------------------------|\n","| Simple, easy to understand and implement              | Assumes linear relationship between features and outcome |\n","| Provides interpretable results (coefficients)         | Sensitive to feature scaling                             |\n","| Efficient for linearly separable data                 | Struggles with complex decision boundaries (non-linear)  |\n","| Provides probability estimates for predictions        | Can underperform with high-dimensional categorical data |\n","| Less prone to overfitting with regularization         | Assumes features are independent (multicollinearity issues) |\n","| Works well with both binary and multiclass problems   | May be too simplistic for complex tasks                 |\n","| Scalable to large datasets                            | May not handle complex feature interactions effectively |\n","\n","---\n","\n","### **When to Use Logistic Regression:**\n","\n","- When your data is linearly separable or approximately linear.\n","- When you need an interpretable model that provides clear insights into how the input features influence the predictions.\n","- For small to medium-sized datasets or when **multiclass** classification is required.\n","- When you are okay with a relatively simple model, and the computational cost of traning."],"metadata":{"id":"dhYXEGIIos5Z"}},{"cell_type":"markdown","source":["## **Que :- 17 What are some use cases of Logistic Regression.**\n","\n","Ans:- **Logistic Regression** is a versatile and widely used machine learning algorithm, particularly for classification tasks. Here are some common use cases where **Logistic Regression** is often applied:\n","\n","---\n","\n","### **1. Medical Diagnosis (Binary Classification)**\n","\n","- **Use Case**: Predicting whether a patient has a certain disease (e.g., **cancer**, **diabetes**) based on medical data such as age, gender, test results, family history, etc.\n","- **Example**: Classifying whether a tumor is malignant or benign based on features like tumor size, shape, and other relevant factors.\n","- **Why Logistic Regression?**: It's easy to interpret, and the probabilistic nature of the model allows doctors to understand the likelihood of the disease, which is helpful in medical decision-making.\n","\n","---\n","\n","### **2. Credit Scoring and Fraud Detection**\n","\n","- **Use Case**: Predicting whether a loan applicant will default on a loan, or if a transaction is fraudulent.\n","- **Example**: Classifying whether a credit card transaction is legitimate or fraudulent based on transaction amount, location, time, and customer history.\n","- **Why Logistic Regression?**: Logistic regression provides clear probability estimates, which are useful in financial services for risk assessment and fraud prevention.\n","\n","---\n","\n","### **3. Customer Churn Prediction**\n","\n","- **Use Case**: Identifying customers who are likely to leave a service or subscription (e.g., telecom, streaming services, or retail) based on usage patterns, customer service interactions, and other factors.\n","- **Example**: Predicting whether a customer will cancel their subscription or churn based on data such as monthly spending, frequency of service usage, and customer feedback.\n","- **Why Logistic Regression?**: The simplicity of logistic regression allows companies to easily understand the factors influencing churn and target retention efforts more effectively.\n","\n","---\n","\n","### **4. Marketing and Customer Segmentation**\n","\n","- **Use Case**: Predicting whether a customer will respond to a marketing campaign or buy a product based on demographic information, previous purchases, and online behavior.\n","- **Example**: Predicting whether a customer will click on an ad or make a purchase based on their interaction history, location, and preferences.\n","- **Why Logistic Regression?**: It’s useful for **binary classification** (e.g., response/no response) and allows marketers to focus efforts on the most promising customers.\n","\n","---\n","\n","### **5. Sentiment Analysis (Text Classification)**\n","\n","- **Use Case**: Classifying the sentiment of customer reviews, social media posts, or comments into positive, negative, or neutral.\n","- **Example**: Predicting whether a customer review on a product is positive or negative based on keywords and sentiment indicators in the text.\n","- **Why Logistic Regression?**: Text data can be transformed into numerical features (e.g., using **TF-IDF** or **word embeddings**), and logistic regression is effective for binary classification tasks like sentiment analysis.\n","\n","---\n","\n","### **6. Email Spam Detection**\n","\n","- **Use Case**: Identifying whether an email is spam or not based on content, sender, and other features.\n","- **Example**: Classifying whether an email is spam based on features such as the frequency of certain words (like “free” or “offer”), the subject line, and the email’s sender.\n","- **Why Logistic Regression?**: It works well for binary classification tasks, and feature engineering can help turn email attributes into a set of useful predictors for spam filtering.\n","\n","---\n","\n","### **7. Marketing Response Prediction (Lead Conversion)**\n","\n","- **Use Case**: Predicting whether a lead or prospect is likely to convert into a paying customer, based on their actions or behaviors.\n","- **Example**: Classifying leads as likely or unlikely to convert into customers based on their activity (such as opening emails, downloading resources, etc.) and demographic information.\n","- **Why Logistic Regression?**: It allows businesses to understand the probability of conversion and allocate resources to the most promising leads.\n","\n","---\n","\n","### **8. Web Page Classification (Click-Through Prediction)**\n","\n","- **Use Case**: Predicting whether a user will click on a particular link, ad, or content on a website based on user behavior, demographics, and content features.\n","- **Example**: Predicting whether a user will click on an ad based on features like the time spent on the page, age, device, and previous interactions.\n","- **Why Logistic Regression?**: Logistic regression can be easily adapted to this type of prediction task, providing probabilities for click-through and helping optimize advertising strategies.\n","\n","---\n","\n","### **9. Employee Attrition Prediction**\n","\n","- **Use Case**: Predicting whether an employee will leave the organization based on their demographic and work performance data.\n","- **Example**: Classifying whether an employee will leave or stay based on factors like job satisfaction, salary, performance ratings, and tenure.\n","- **Why Logistic Regression?**: Logistic regression provides a simple approach to predicting employee attrition, and the results are interpretable, which is valuable for HR decisions.\n","\n","---\n","\n","### **10. Election Prediction (Political Polling)**\n","\n","- **Use Case**: Predicting election results based on voting behavior and demographic data.\n","- **Example**: Classifying whether a voter will vote for a certain candidate based on age, income, education level, and previous voting history.\n","- **Why Logistic Regression?**: It’s useful for estimating the probability of a voter supporting a particular candidate, which can guide campaigns.\n","\n","---\n","\n","### **11. Disease Outbreak Prediction (Epidemiology)**\n","\n","- **Use Case**: Predicting the likelihood of a disease outbreak based on environmental, social, and medical factors.\n","- **Example**: Predicting the likelihood of an outbreak of an infectious disease like the flu based on variables such as population density, previous outbreaks, and vaccination rates.\n","- **Why Logistic Regression?**: Logistic regression can help identify the probability of an outbreak and is simple to implement for early warning systems.\n","\n","---\n","\n","### **12. Risk Management (Insurance)**\n","\n","- **Use Case**: Predicting the risk of certain events, such as accidents, or insurance claims based on historical data.\n","- **Example**: Predicting whether an individual will file an insurance claim based on their profile and driving history in the case of auto insurance.\n","- **Why Logistic Regression?**: It helps insurance companies assess the likelihood of claims and adjust premiums accordingly.\n","\n","---\n","\n","### **In Summary:**\n","Logistic Regression is useful when:\n","- You need a **probabilistic interpretation** of predictions (likelihood of belonging to a class).\n","- The relationship between the input features and the outcome is assumed to be **linear**.\n","- The data is **binary** or can be transformed into binary (using techniques like **One-vs-Rest** for multiclass problems).\n","- You need an **easy-to-interpret** model with a clear understanding of feature importance."],"metadata":{"id":"s8lsBcF0pH7I"}},{"cell_type":"markdown","source":["## **Que :- 18 What is the difference between Softmax Regression and Logistic Regression.**\n","\n","**Softmax Regression** and **Logistic Regression** are both types of **generalized linear models** used for classification, but they differ in how they handle the number of classes they can predict. Here’s a detailed breakdown of the differences between **Softmax Regression** (also known as **Multinomial Logistic Regression**) and **Logistic Regression**:\n","\n","---\n","\n","### **1. Problem Type: Binary vs. Multiclass**\n","\n","- **Logistic Regression**:\n","  - **Used for binary classification** problems, where the outcome variable has only **two possible classes** (e.g., \"Yes\" vs. \"No\", \"True\" vs. \"False\").\n","  - The model predicts the **probability** of an observation belonging to one of two classes, typically using the **sigmoid function**.\n","  - Example: Predicting whether an email is spam or not (2 classes).\n","\n","- **Softmax Regression (Multinomial Logistic Regression)**:\n","  - **Used for multiclass classification** problems, where the outcome variable has **more than two classes** (e.g., \"Cat\", \"Dog\", \"Fish\").\n","  - The model computes **probabilities** for each of the multiple classes and predicts the class with the highest probability. It uses the **softmax function** for this.\n","  - Example: Classifying images into one of several categories, like \"cat\", \"dog\", or \"bird\" (3 or more classes).\n","\n","---\n","\n","### **2. Mathematical Approach**\n","\n","- **Logistic Regression**:\n","  - The model predicts the **log-odds** (logit) of the event belonging to a particular class (usually class 1) and uses the **sigmoid function** to map the result to a probability between 0 and 1.\n","  \n","  \\[\n","  P(y = 1 | X) = \\frac{1}{1 + e^{-(w_0 + w_1 x_1 + \\dots + w_n x_n)}}\n","  \\]\n","\n","  Where:\n","  - \\(P(y = 1 | X)\\) is the probability of class 1 (the event occurring),\n","  - \\(x_1, x_2, \\dots, x_n\\) are the input features,\n","  - \\(w_0, w_1, \\dots, w_n\\) are the model parameters.\n","\n","- **Softmax Regression**:\n","  - Softmax Regression generalizes logistic regression for **multiclass classification** by using the **softmax function** to calculate probabilities for each class.\n","  \n","  \\[\n","  P(y = k | X) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}\n","  \\]\n","\n","  Where:\n","  - \\(P(y = k | X)\\) is the probability of class \\(k\\),\n","  - \\(\\theta_k\\) are the parameters associated with class \\(k\\),\n","  - \\(x\\) is the feature vector,\n","  - \\(K\\) is the total number of classes.\n","\n","  This allows for the prediction of the **probabilities for multiple classes** (not just two), and the class with the highest probability is chosen.\n","\n","---\n","\n","### **3. Output**\n","\n","- **Logistic Regression**:\n","  - Outputs a single **probability** value between 0 and 1 for binary classification.\n","  - The output is typically interpreted as the probability of the positive class (class 1).\n","  \n","- **Softmax Regression**:\n","  - Outputs a **probability distribution** across all possible classes.\n","  - The sum of the probabilities for all classes is **1** (since it's a probability distribution), and the class with the highest probability is selected as the predicted class.\n","\n","---\n","\n","### **4. Decision Boundary**\n","\n","- **Logistic Regression**:\n","  - The decision boundary is **linear**. It separates the two classes with a straight line (in 2D) or a hyperplane (in higher dimensions).\n","\n","- **Softmax Regression**:\n","  - The decision boundaries are also **linear**, but there are as many decision boundaries as there are classes. Essentially, each class gets its own linear decision boundary.\n","\n","---\n","\n","### **5. Use Cases**\n","\n","- **Logistic Regression**:\n","  - Best for **binary classification** tasks.\n","  - Examples: Spam detection (spam vs. non-spam), disease diagnosis (diseased vs. healthy), click-through prediction (clicked vs. not clicked).\n","  \n","- **Softmax Regression**:\n","  - Best for **multiclass classification** tasks where there are more than two possible outcomes.\n","  - Examples: Handwriting recognition (digits 0-9), image classification (e.g., dog, cat, bird), multi-class sentiment analysis (positive, negative, neutral).\n","\n","---\n","\n","### **6. Model Training**\n","\n","- **Logistic Regression**:\n","  - Can be trained with **binary labels** (0 or 1) using algorithms like **gradient descent** or **Newton's method**.\n","  \n","- **Softmax Regression**:\n","  - Requires **one-hot encoded labels** for each class (a vector where one element is 1 and the rest are 0), and is trained using **gradient descent** or other optimization methods suitable for multiclass problems.\n","\n","---\n","\n","### **7. Regularization**\n","\n","- Both **Logistic Regression** and **Softmax Regression** can use **regularization** to prevent overfitting:\n","  - **L1 regularization** (Lasso) or **L2 regularization** (Ridge) can be applied to the model to penalize large coefficients and improve generalization.\n","  \n","---\n","\n","### **Summary of Differences**:\n","\n","| **Feature**                  | **Logistic Regression**                                  | **Softmax Regression**                                 |\n","|------------------------------|----------------------------------------------------------|--------------------------------------------------------|\n","| **Problem Type**             | Binary classification (2 classes)                        | Multiclass classification (3 or more classes)           |\n","| **Mathematical Approach**    | Sigmoid function for binary probabilities                | Softmax function for multiclass probabilities           |\n","| **Output**                   | Single probability (class 1 vs. class 0)                 | Probability distribution across all classes             |\n","| **Decision Boundary**        | Single linear boundary between two classes               | Multiple linear boundaries, one for each class         |\n","| **Use Cases**                | Binary tasks (e.g., spam detection, disease diagnosis)   | Multiclass tasks (e.g., image classification, sentiment analysis) |\n","| **Training**                 | Binary labels (0 or 1)                                   | One-hot encoded labels for multiple classes             |\n","\n","---\n","\n","### **When to Use Each Model:**\n","\n","- **Use Logistic Regression** if your task involves **binary classification** (e.g., predicting two classes, such as \"Yes\" or \"No\").\n","- **Use Softmax Regression** if you need to handle **multiclass classification** (e.g., more than two possible outcomes, like categorizing images into different type.|"],"metadata":{"id":"Rb2I5HVTphSq"}},{"cell_type":"markdown","source":["## **Que :- 19 How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.**\n","\n","Ans:- When it comes to multiclass classification, both **One-vs-Rest (OvR)** and **Softmax Regression** are viable approaches. The choice between these two methods depends on factors such as the complexity of the problem, model behavior, and the specifics of the dataset. Here's a detailed comparison and guidelines for when to choose **One-vs-Rest (OvR)** or **Softmax**:\n","\n","---\n","\n","### **1. One-vs-Rest (OvR) Approach:**\n","In the **One-vs-Rest (OvR)** approach, a separate binary classifier is trained for each class. For each classifier, the goal is to distinguish one class from all other classes. When making predictions, the classifier with the highest confidence for a class will make the final decision.\n","\n","- **How it works**:\n","  - For \\( K \\) classes, you train \\( K \\) binary classifiers.\n","  - Each classifier outputs a probability of the input belonging to its corresponding class vs. all others (binary).\n","  - The final prediction is based on the classifier with the highest probability.\n","\n","---\n","\n","### **2. Softmax Regression (Multinomial Logistic Regression):**\n","In **Softmax Regression**, a single model is trained to predict the probability distribution over all classes at once. It directly computes the probability of each class based on the input features and the softmax function.\n","\n","- **How it works**:\n","  - Softmax gives a probability distribution over the \\( K \\) classes for each input, and the class with the highest probability is chosen.\n","  - It is a **single model** trained to handle multiple classes simultaneously, rather than multiple binary classifiers.\n","\n","---\n","\n","### **Key Differences Between OvR and Softmax**:\n","\n","| **Feature**                       | **One-vs-Rest (OvR)**                                      | **Softmax Regression**                                     |\n","|-----------------------------------|------------------------------------------------------------|------------------------------------------------------------|\n","| **Number of Models**              | \\( K \\) binary classifiers (one for each class)            | Single model for all classes                                |\n","| **Training Complexity**           | Requires training \\( K \\) classifiers separately            | Requires training a single model with softmax function      |\n","| **Prediction**                    | Predicts class with the highest output from the \\( K \\) classifiers | Predicts class with the highest probability from softmax     |\n","| **Model Interpretation**          | Each classifier gives a separate decision boundary          | Direct interpretation of class probabilities                |\n","| **Output**                        | Each classifier outputs a binary decision (1 or 0) for its class | Softmax gives a probability distribution over all classes   |\n","| **Suitability for Imbalanced Data**| Can struggle with imbalanced classes                       | Can handle imbalanced classes better with proper regularization |\n","| **Training Time**                 | \\( K \\) separate models can increase training time         | Single model training may be more efficient                 |\n","| **Overfitting Risk**              | Higher risk of overfitting due to separate models for each class | Lower risk due to shared parameters across classes          |\n","\n","---\n","\n","### **When to Use One-vs-Rest (OvR):**\n","\n","1. **When you have a simple problem** with **linearly separable classes**:\n","   - OvR is often simpler to implement and may be efficient for simple problems with a **small number of classes**.\n","   \n","2. **When individual classifiers are interpretable**:\n","   - Each classifier in the OvR approach focuses on distinguishing one class from the others, which can be useful for understanding class boundaries, especially when the number of classes is small.\n","   \n","3. **When computational efficiency is a concern**:\n","   - If you're dealing with a **very large number of classes**, the OvR approach can sometimes be more **computationally efficient**. It allows for training multiple smaller models rather than a large, more complex model.\n","\n","4. **In the case of **imbalanced datasets**:\n","   - If the classes are imbalanced, OvR may still perform decently by training each binary classifier independently, potentially allowing it to focus on one class at a time.\n","\n","---\n","\n","### **When to Use Softmax Regression:**\n","\n","1. **When you have a large number of classes**:\n","   - Softmax is often preferred when dealing with a large number of classes because it **directly models the probability distribution** over all classes in a single step, avoiding the complexity of multiple classifiers.\n","   \n","2. **When you want to model the **interdependence between classes**:\n","   - Softmax Regression models the relationship between classes in a single model, which can be beneficial when the classes are not completely independent of each other.\n","   - The model's softmax function ensures that class probabilities sum to 1, providing a natural way to interpret class predictions in terms of probabilities.\n","   \n","3. **When you have **improved accuracy requirements**:\n","   - Softmax tends to be better suited for datasets where the classes are **not linearly separable** because it directly learns the relationship between features and all classes simultaneously.\n","\n","4. **When you need more **robustness** to imbalanced data**:\n","   - **Softmax Regression** can handle class imbalances better by incorporating class probabilities and optimizing across all classes simultaneously.\n","   \n","5. **For multi-class classification with a natural hierarchy**:\n","   - If you’re working with datasets where classes have a **hierarchical structure** (e.g., \"low\", \"medium\", \"high\"), Softmax may model these relationships better than OvR.\n","\n","---\n","\n","### **Other Considerations:**\n","\n","- **Handling Imbalance**:\n","  - **Softmax Regression** may be more robust to class imbalance than **OvR** because it models all classes together, but both approaches can benefit from **class weighting** or **regularization** when dealing with imbalanced datasets.\n","\n","- **Performance**:\n","  - In general, if the **classes are well-separated** and independent, **OvR** can sometimes outperform Softmax.\n","  - However, if there's a lot of **overlap** between classes or if the dataset is complex, **Softmax** tends to perform better since it considers all classes simultaneously in a unified model.\n","\n","---\n","\n","### **Summary:**\n","\n","| **Scenario**                                      | **Use One-vs-Rest (OvR)**                            | **Use Softmax Regression**                             |\n","|---------------------------------------------------|------------------------------------------------------|--------------------------------------------------------|\n","| **Few Classes and Simple Problem**                | Suitable for binary-like or small datasets           | May be overkill for simpler problems                   |\n","| **Computational Efficiency**                      | More efficient with a smaller number of classes      | More computationally expensive (due to single model)   |\n","| **Class Interdependence**                         | Not suitable for interdependent classes              | Suitable for capturing interdependence between classes |\n","| **Imbalanced Data**                               | Can perform well with proper regularization          | Can be more robust with class balancing                |\n","| **High Complexity or Large Number of Classes**    | Can become inefficient                             | More suitable for handling large numbers of classes    |\n","| **Interpretability**                              | Easier to interpret class-wise decision boundaries   | Direct probability prediction (less interpretability)  |\n","\n","---\n","\n","### **Conclusion:**\n","\n","- **Choose OvR** when you have relatively **simple** problems with a small number of classes, and when you need **interpretability** from individual classifiers.\n","- **Choose Softmax** when you’re dealing with **more complex problems**, a **larger number of classes**, or if you need to consider **class interdependencies** or **probability distributions**."],"metadata":{"id":"w8Iit4g3pzmu"}},{"cell_type":"markdown","source":["## **Que :- 20 How do we interpret coefficients in Logistic Regression?**\n","\n","Ans:- Interpreting the coefficients in **Logistic Regression** is an important step to understand the relationship between the predictor variables (features) and the outcome variable (target). The coefficients tell us how each feature impacts the odds of the event happening (e.g., class 1 in a binary classification task).\n","\n","### **1. Logistic Regression Model Recap**\n","\n","In Logistic Regression, the model predicts the probability of an event occurring, based on the following equation:\n","\n","\\[\n","P(y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n","\\]\n","\n","Where:\n","- \\( P(y = 1 | X) \\) is the probability of the event (e.g., class 1),\n","- \\( \\beta_0 \\) is the **intercept** (the log-odds when all features are 0),\n","- \\( \\beta_1, \\dots, \\beta_n \\) are the **coefficients** for the features \\( x_1, \\dots, x_n \\),\n","- \\( x_1, \\dots, x_n \\) are the feature values.\n","\n","---\n","\n","### **2. Interpreting the Coefficients:**\n","\n","The key to interpreting the coefficients is understanding the relationship between the predictor and the log-odds of the outcome.\n","\n","- **Sign of the Coefficient (\\( \\beta_i \\)):**\n","  - A **positive coefficient** \\( \\beta_i \\) indicates that as the feature \\( x_i \\) increases, the **log-odds** of the outcome (e.g., class 1) increase, which means the probability of class 1 increases.\n","  - A **negative coefficient** \\( \\beta_i \\) indicates that as the feature \\( x_i \\) increases, the log-odds of the outcome decrease, meaning the probability of class 1 decreases.\n","\n","- **Magnitude of the Coefficient:**\n","  - The **magnitude** of \\( \\beta_i \\) indicates the strength of the relationship between the feature \\( x_i \\) and the outcome. Larger absolute values of \\( \\beta_i \\) indicate a stronger relationship.\n","  - The magnitude can also give an indication of how sensitive the probability of the outcome is to changes in that feature.\n","\n","---\n","\n","### **3. Interpreting Coefficients in Terms of Odds:**\n","\n","Since Logistic Regression outputs probabilities, we often find it easier to interpret the coefficients in terms of **odds** rather than log-odds.\n","\n","#### **Odds Ratio:**\n","\n","The **odds ratio (OR)** is the exponential of the coefficient:\n","\n","\\[\n","OR = e^{\\beta_i}\n","\\]\n","\n","- An **odds ratio greater than 1** indicates that as \\( x_i \\) increases, the odds of the event occurring increase (i.e., the probability of class 1 increases).\n","- An **odds ratio less than 1** indicates that as \\( x_i \\) increases, the odds of the event occurring decrease (i.e., the probability of class 1 decreases).\n","- An **odds ratio of 1** means that \\( x_i \\) has no effect on the odds of the event (the feature doesn't influence the probability).\n","\n","#### **Example Interpretation**:\n","\n","- Suppose the coefficient for a feature \\( x_1 \\) is \\( \\beta_1 = 0.5 \\).\n","  - The **odds ratio** would be \\( e^{0.5} \\approx 1.65 \\).\n","  - This means that for a one-unit increase in \\( x_1 \\), the odds of the outcome (class 1) increase by **65%**.\n","\n","- If the coefficient for a feature \\( x_2 \\) is \\( \\beta_2 = -0.3 \\):\n","  - The **odds ratio** would be \\( e^{-0.3} \\approx 0.74 \\).\n","  - This means that for a one-unit increase in \\( x_2 \\), the odds of the outcome (class 1) decrease by about **26%**.\n","\n","---\n","\n","### **4. Interpreting the Intercept:**\n","\n","The **intercept** \\( \\beta_0 \\) represents the **log-odds** of the outcome when all predictor variables \\( x_1, x_2, \\dots, x_n \\) are zero. The interpretation of the intercept on its own is often not very meaningful unless you have context around what happens when all the predictors are zero.\n","\n","For instance:\n","- If the intercept is \\( \\beta_0 = -2 \\), then when all features are zero, the log-odds of the outcome being class 1 are -2. To convert this to the odds and probability:\n","  - **Odds**: \\( e^{-2} \\approx 0.135 \\) (the odds of the outcome are about 0.135),\n","  - **Probability**: \\( P(y = 1) = \\frac{1}{1 + e^2} \\approx 0.12 \\) (the probability of class 1 is 12%).\n","\n","---\n","\n","### **5. Example in Practice:**\n","\n","Consider the following logistic regression output:\n","\n","| Feature       | Coefficient (β) | Odds Ratio (e^β) |\n","|---------------|-----------------|------------------|\n","| Age           | 0.05            | 1.05             |\n","| Income        | -0.03           | 0.97             |\n","| Education     | 0.10            | 1.11             |\n","| Intercept     | -2.00           | N/A              |\n","\n","- **Age**: For every 1-year increase in age, the odds of the person being classified as \"Class 1\" increase by **5%**.\n","- **Income**: For every 1-unit increase in income, the odds of the person being classified as \"Class 1\" decrease by **3%**.\n","- **Education**: For every 1-unit increase in education (perhaps measured in years of schooling), the odds of the person being classified as \"Class 1\" increase by **11%**.\n","- **Intercept**: When all features (Age, Income, Education) are 0, the log-odds of being in \"Class 1\" is -2. The odds of being in \"Class 1\" are \\( e^{-2} \\approx 0.135 \\), which corresponds to a probability of \\( P(y = 1) = 0.12 \\).\n","\n","---\n","\n","### **6. Log-odds vs. Probability:**\n","\n","- The coefficients are **log-odds**: Each coefficient represents how much the **log-odds** of the outcome increase or decrease with a one-unit change in the predictor.\n","- To **convert log-odds to probabilities**, use the logistic function:\n","\n","\\[\n","P(y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n","\\]\n","\n","This function maps the log-odds (which can range from \\(-\\infty\\) to \\(+\\infty\\)) to a probability range of 0 to 1.\n","\n","---\n","\n","### **7. Summary:**\n","\n","- **Positive coefficients** increase the probability of the outcome (class 1).\n","- **Negative coefficients** decrease the probability of the outcome (class 1).\n","- **Magnitude of coefficients** indicates the strength of the effect of a feature on the log-odds.\n","- **Odds ratio** ( \\( e^{\\beta} \\) ) makes it easier to interpret the effect of each feature on the outcome.\n","- The **intercept** is the log-odds of the event when all features are 0, but its interpretation is often less meaningful unless features have zero values in your dataset."],"metadata":{"id":"nug7sklLqTuj"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"gCSLn3KdiPl3","executionInfo":{"status":"ok","timestamp":1739874717953,"user_tz":-330,"elapsed":446,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# **PRACTICAL QUESTIONS**"],"metadata":{"id":"6CE6-7EtrnIA"}},{"cell_type":"markdown","source":["## **Que :- 1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.**"],"metadata":{"id":"gc9Z1oOQrxFF"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"wBU7Rgo8nKw-","executionInfo":{"status":"ok","timestamp":1739876009057,"user_tz":-330,"elapsed":369,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load dataset (in this case, we'll use the Iris dataset for multiclass classification)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model\n","model = LogisticRegression(max_iter=200)\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGHIOtmLsQB0","executionInfo":{"status":"ok","timestamp":1739876109216,"user_tz":-330,"elapsed":398,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"b8f28d44-2c14-4cea-e809-78de23c5bd42"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.**"],"metadata":{"id":"EbdHsELqss64"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load dataset (in this case, we'll use the Iris dataset for multiclass classification)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model with L1 regularization (Lasso)\n","model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(f\"Model Accuracy with L1 Regularization (Lasso): {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vN-PCZiGsa4A","executionInfo":{"status":"ok","timestamp":1739876164452,"user_tz":-330,"elapsed":399,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"f48126e5-d6c7-4e05-bbe3-00f1746c4283"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy with L1 Regularization (Lasso): 100.00%\n"]}]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"gkojjLCss186","executionInfo":{"status":"ok","timestamp":1739876191494,"user_tz":-330,"elapsed":413,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"1394bb56-74ec-462e-d642-1d305d82861d"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(max_iter=200, penalty='l1', solver='liblinear')"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=200, penalty=&#x27;l1&#x27;, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## **Que :- 3 Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.**"],"metadata":{"id":"0mK0XtjEtEOj"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load dataset (using the Iris dataset for multiclass classification)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model with L2 regularization (Ridge)\n","model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(f\"Model Accuracy with L2 Regularization (Ridge): {accuracy * 100:.2f}%\")\n","\n","# Print the coefficients\n","print(\"\\nModel Coefficients (for each class):\")\n","print(model.coef_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fU41u4u2s8ZT","executionInfo":{"status":"ok","timestamp":1739876307858,"user_tz":-330,"elapsed":965,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"d786fdbf-4d7a-4386-ccf8-bc46874a27ed"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy with L2 Regularization (Ridge): 100.00%\n","\n","Model Coefficients (for each class):\n","[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n"," [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n"," [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"]}]},{"cell_type":"markdown","source":["## **Que :- 4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')**"],"metadata":{"id":"a7sFpMuctfIz"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load dataset (using the Iris dataset for multiclass classification)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model with Elastic Net Regularization\n","model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=200, l1_ratio=0.5)\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(f\"Model Accuracy with Elastic Net Regularization: {accuracy * 100:.2f}%\")\n","\n","# Print the coefficients\n","print(\"\\nModel Coefficients (for each class):\")\n","print(model.coef_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NaPe3S6mtYjo","executionInfo":{"status":"ok","timestamp":1739876364241,"user_tz":-330,"elapsed":410,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"d4e892b0-ecc9-4d60-de35-2b04eadf6599"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy with Elastic Net Regularization: 100.00%\n","\n","Model Coefficients (for each class):\n","[[ 0.38842396  1.77443954 -2.4221925  -0.70575722]\n"," [ 0.07494495  0.          0.         -0.58001565]\n"," [-1.25710564 -1.52593344  2.59624313  2.0796999 ]]\n"]}]},{"cell_type":"markdown","source":["## **Que :-5 Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.**"],"metadata":{"id":"OZa_LQQit1bP"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","\n","# Load dataset (using the Iris dataset for multiclass classification)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model with One-vs-Rest (OvR) strategy\n","model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the accuracy\n","print(f\"Model Accuracy with One-vs-Rest (OvR) Strategy: {accuracy * 100:.2f}%\")\n","\n","# Print the coefficients (one set for each class)\n","print(\"\\nModel Coefficients (for each class):\")\n","print(model.coef_)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYnhuLNwtmdY","executionInfo":{"status":"ok","timestamp":1739876476828,"user_tz":-330,"elapsed":406,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"a84dd891-6a8f-4977-d3ee-4f4916a000b1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy with One-vs-Rest (OvR) Strategy: 100.00%\n","\n","Model Coefficients (for each class):\n","[[ 0.3711229   1.409712   -2.15210117 -0.95474179]\n"," [ 0.49400451 -1.58897112  0.43717015 -1.11187838]\n"," [-1.55895271 -1.58893375  2.39874554  2.15556209]]\n"]}]},{"cell_type":"markdown","source":["## **Que :- 6 \"C Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.**"],"metadata":{"id":"fuAIB5ONuEE8"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset (using the Iris dataset for multiclass classification)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model\n","model = LogisticRegression(max_iter=200)\n","\n","# Define the parameter grid for hyperparameter tuning (C and penalty)\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n","    'penalty': ['l1', 'l2'],  # Regularization type\n","}\n","\n","# Initialize GridSearchCV to search over the hyperparameter space\n","grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n","\n","# Fit the model using GridSearchCV on the training data\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters from the grid search\n","best_params = grid_search.best_params_\n","\n","# Print the best hyperparameters\n","print(f\"Best Hyperparameters: {best_params}\")\n","\n","# Make predictions on the test data using the best model\n","y_pred = grid_search.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the model accuracy\n","print(f\"Model Accuracy with the Best Hyperparameters: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6q5wIiGguQNz","executionInfo":{"status":"ok","timestamp":1739876548746,"user_tz":-330,"elapsed":1082,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"465bf755-0dca-43c1-8ade-0f4c7455b291"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Hyperparameters: {'C': 1, 'penalty': 'l2'}\n","Model Accuracy with the Best Hyperparameters: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 7 Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.**"],"metadata":{"id":"HSUdOllDuWWB"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","from sklearn.model_selection import StratifiedKFold, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","\n","# Load dataset (using the Iris dataset for multiclass classification)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Initialize the Logistic Regression model\n","model = LogisticRegression(max_iter=200)\n","\n","# Initialize Stratified K-Fold Cross-Validation (5 splits)\n","stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Perform cross-validation and get the accuracy for each fold\n","cv_scores = cross_val_score(model, X, y, cv=stratified_kfold, scoring='accuracy')\n","\n","# Print the accuracy for each fold\n","print(f\"Accuracy for each fold: {cv_scores}\")\n","\n","# Calculate and print the average accuracy across all folds\n","average_accuracy = np.mean(cv_scores)\n","print(f\"Average Accuracy: {average_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUBeHFP-uTVt","executionInfo":{"status":"ok","timestamp":1739876640870,"user_tz":-330,"elapsed":437,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"0c68f1d7-67ab-4c22-fd39-0673b487872f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy for each fold: [1.         0.96666667 0.93333333 1.         0.93333333]\n","Average Accuracy: 96.67%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 8 Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.**"],"metadata":{"id":"l8ZiCKlVu-wg"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_breast_cancer\n","\n","# Load the Breast Cancer dataset from sklearn (you can replace this with your own CSV dataset)\n","data = load_breast_cancer()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model\n","model = LogisticRegression(max_iter=200)\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the model accuracy\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75ysp5ANuqQF","executionInfo":{"status":"ok","timestamp":1739877419265,"user_tz":-330,"elapsed":473,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"e98c576b-a973-4bbe-a682-77c98cb0cb7d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 95.61%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 9 Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.**"],"metadata":{"id":"D3aNAXDKxvQt"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from scipy.stats import uniform\n","\n","# Load the Iris dataset (you can replace this with your own dataset)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Define the Logistic Regression model\n","model = LogisticRegression(max_iter=200)\n","\n","# Define the hyperparameter distribution\n","param_dist = {\n","    'C': uniform(0.001, 1000),  # Randomly sampling C in this range\n","    'penalty': ['l1', 'l2'],    # Regularization penalty\n","    'solver': ['liblinear', 'saga']  # Solvers to choose from\n","}\n","\n","# Initialize RandomizedSearchCV\n","random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n","                                   n_iter=10, cv=5, random_state=42)\n","\n","# Fit the RandomizedSearchCV model on the training data\n","random_search.fit(X_train, y_train)\n","\n","# Print the best hyperparameters found by RandomizedSearchCV\n","print(f\"Best Hyperparameters: {random_search.best_params_}\")\n","\n","# Make predictions on the test data using the best model\n","y_pred = random_search.best_estimator_.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the model accuracy\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_NkMjAOvJqz","executionInfo":{"status":"ok","timestamp":1739877501002,"user_tz":-330,"elapsed":2213,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"8e6f478d-4b83-43e2-dbc3-dd18300c9ffd"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Hyperparameters: {'C': 601.1160117432088, 'penalty': 'l2', 'solver': 'liblinear'}\n","Model Accuracy: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.**"],"metadata":{"id":"Z6TbSa9Vx-Vt"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multiclass import OneVsOneClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset (you can replace this with your own dataset)\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Use OneVsOneClassifier with Logistic Regression as the base estimator\n","ovo_classifier = OneVsOneClassifier(log_reg)\n","\n","# Train the model on the training data\n","ovo_classifier.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = ovo_classifier.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the model accuracy\n","print(f\"One-vs-One Logistic Regression Model Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rLTq0vtXwt8r","executionInfo":{"status":"ok","timestamp":1739877596088,"user_tz":-330,"elapsed":407,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"5636ebc1-8ca5-4855-a4ae-8b0b0aecdd1e"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["One-vs-One Logistic Regression Model Accuracy: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 11 Write a Python program to train a LogisticRegression model and visualize the confusion matrix for binary classification.**"],"metadata":{"id":"-Bq0nwHtyUG9"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","\n","# Load the Breast Cancer dataset (binary classification problem)\n","data = load_breast_cancer()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the model on the training data\n","log_reg.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = log_reg.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Visualize the confusion matrix using seaborn\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted: 0', 'Predicted: 1'], yticklabels=['Actual: 0', 'Actual: 1'])\n","plt.title(\"Confusion Matrix for Logistic Regression\")\n","plt.ylabel('Actual Label')\n","plt.xlabel('Predicted Label')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"id":"biUnClpLxAh0","executionInfo":{"status":"ok","timestamp":1739877662412,"user_tz":-330,"elapsed":1703,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"6886d6ee-160d-445f-bc8a-058d5ae00aae"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 95.61%\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 600x400 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## **Que :- 12 Write a Python program to train a LogisticRegression model and evaluate its performance using Precision,Recall, and F1-Score.**"],"metadata":{"id":"4VXvL6qByrCd"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","# Load the Breast Cancer dataset (binary classification problem)\n","data = load_breast_cancer()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the model on the training data\n","log_reg.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = log_reg.predict(X_test)\n","\n","# Calculate Accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Calculate Precision\n","precision = precision_score(y_test, y_pred)\n","print(f\"Precision: {precision:.2f}\")\n","\n","# Calculate Recall\n","recall = recall_score(y_test, y_pred)\n","print(f\"Recall: {recall:.2f}\")\n","\n","# Calculate F1-Score\n","f1 = f1_score(y_test, y_pred)\n","print(f\"F1-Score: {f1:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ef3VSx75yjVo","executionInfo":{"status":"ok","timestamp":1739877752196,"user_tz":-330,"elapsed":368,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"f8d7e2e7-485f-4910-a429-f3a24693bd1f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 95.61%\n","Precision: 0.95\n","Recall: 0.99\n","F1-Score: 0.97\n"]}]},{"cell_type":"markdown","source":["## **Que :- 13 Write a Python program to train a LogisticRegression model on imbalanced data and apply class weights to improve model performance.**"],"metadata":{"id":"d7dY6QFmy7i9"}},{"cell_type":"code","source":["# Import necessary libraries\n","import numpy as np\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Load the Breast Cancer dataset (binary classification problem)\n","data = load_breast_cancer()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Simulate an imbalanced dataset by making one class less frequent\n","# Let's make the 'malignant' class (1) appear 80% of the time and 'benign' (0) only 20% of the time\n","y_imbalanced = np.copy(y)\n","y_imbalanced[:int(len(y) * 0.8)] = 1  # Change 80% of the labels to 1 (malignant)\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_imbalanced, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model with class weights\n","log_reg = LogisticRegression(class_weight='balanced', max_iter=200)\n","\n","# Train the model on the training data\n","log_reg.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = log_reg.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Print classification report (Precision, Recall, F1-Score for each class)\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TY0ZP9tsy5lY","executionInfo":{"status":"ok","timestamp":1739877832501,"user_tz":-330,"elapsed":382,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"3282a305-0ea0-4762-d757-74beeac03f82"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 79.82%\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.21      1.00      0.34         6\n","           1       1.00      0.79      0.88       108\n","\n","    accuracy                           0.80       114\n","   macro avg       0.60      0.89      0.61       114\n","weighted avg       0.96      0.80      0.85       114\n","\n"]}]},{"cell_type":"markdown","source":["## **Que :- 14 Write a Python program to train LogisticRegression on the Titanic dataset, handle missing values, and evaluate performance.**"],"metadata":{"id":"mVQlh5eHzPXr"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the Titanic dataset (replace with your path if needed)\n","url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n","data = pd.read_csv(url)\n","\n","# Display the first few rows of the dataset to understand its structure\n","print(data.head())\n","\n","# Handle missing values (fill missing 'Age' with the median, and 'Embarked' with the mode)\n","data['Age'].fillna(data['Age'].median(), inplace=True)\n","data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n","\n","# Drop columns that are not needed for prediction (e.g., 'Name', 'Ticket', 'Cabin')\n","data = data.drop(columns=['Name', 'Ticket', 'Cabin'])\n","\n","# Convert categorical columns to numerical values using one-hot encoding\n","data = pd.get_dummies(data, columns=['Sex', 'Embarked'], drop_first=True)\n","\n","# Split the data into features (X) and target (y)\n","X = data.drop(columns=['Survived'])  # Features\n","y = data['Survived']  # Target\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# Initialize and train the Logistic Regression model\n","log_reg = LogisticRegression(max_iter=200)\n","log_reg.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = log_reg.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Print classification report (Precision, Recall, F1-Score for each class)\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XbwmnmZzzNMQ","executionInfo":{"status":"ok","timestamp":1739877940454,"user_tz":-330,"elapsed":840,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"24072e28-a9a4-4acd-a80f-8e281827f7cc"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["   PassengerId  Survived  Pclass  \\\n","0            1         0       3   \n","1            2         1       1   \n","2            3         1       3   \n","3            4         1       1   \n","4            5         0       3   \n","\n","                                                Name     Sex   Age  SibSp  \\\n","0                            Braund, Mr. Owen Harris    male  22.0      1   \n","1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n","2                             Heikkinen, Miss. Laina  female  26.0      0   \n","3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n","4                           Allen, Mr. William Henry    male  35.0      0   \n","\n","   Parch            Ticket     Fare Cabin Embarked  \n","0      0         A/5 21171   7.2500   NaN        S  \n","1      0          PC 17599  71.2833   C85        C  \n","2      0  STON/O2. 3101282   7.9250   NaN        S  \n","3      0            113803  53.1000  C123        S  \n","4      0            373450   8.0500   NaN        S  \n","Model Accuracy: 80.45%\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.85      0.84       105\n","           1       0.77      0.74      0.76        74\n","\n","    accuracy                           0.80       179\n","   macro avg       0.80      0.80      0.80       179\n","weighted avg       0.80      0.80      0.80       179\n","\n"]}]},{"cell_type":"markdown","source":["## **Que :- 15 Write a Python program to apply feature scaling (Standardization) before training a LogisticRegression model. Evaluate its accuracy and compare results with and without scaling.**"],"metadata":{"id":"fG-qjBa0ztsr"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Logistic Regression model\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the model without scaling (using raw data)\n","log_reg.fit(X_train, y_train)\n","y_pred_raw = log_reg.predict(X_test)\n","\n","# Calculate accuracy without scaling\n","accuracy_raw = accuracy_score(y_test, y_pred_raw)\n","print(f\"Accuracy without scaling: {accuracy_raw * 100:.2f}%\")\n","\n","# Apply feature scaling (standardization)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Train the model with scaling\n","log_reg.fit(X_train_scaled, y_train)\n","y_pred_scaled = log_reg.predict(X_test_scaled)\n","\n","# Calculate accuracy with scaling\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","print(f\"Accuracy with scaling: {accuracy_scaled * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y3BYTkDDznbn","executionInfo":{"status":"ok","timestamp":1739878028361,"user_tz":-330,"elapsed":365,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"0cef90ea-5c1b-498e-ff0c-f6f62e568f96"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy without scaling: 100.00%\n","Accuracy with scaling: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 16 Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.**"],"metadata":{"id":"nFePuotnz_Hs"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert the target to a binary classification task (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize the Logistic Regression model\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the model\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test data\n","y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class (Setosa)\n","\n","# Calculate the ROC-AUC score\n","roc_auc = roc_auc_score(y_test, y_pred_proba)\n","print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n","\n","# Plot ROC curve\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.4f})')\n","plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.xlabel('False Positive Rate (FPR)')\n","plt.ylabel('True Positive Rate (TPR)')\n","plt.legend(loc='lower right')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"id":"HUL65245z9An","executionInfo":{"status":"ok","timestamp":1739878092707,"user_tz":-330,"elapsed":2124,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"e70e979a-e8fa-41fc-c67d-ee418323b2ab"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["ROC-AUC Score: 1.0000\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["### **Que :- 17 E3M Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluateaccuracy.**"],"metadata":{"id":"OfJdAOBE0UAl"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize Logistic Regression with custom C=0.5 (lower value increases regularization)\n","log_reg = LogisticRegression(C=0.5, max_iter=200)\n","\n","# Train the model\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test data\n","y_pred = log_reg.predict(X_test_scaled)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy with C=0.5: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVejzAbc0L_o","executionInfo":{"status":"ok","timestamp":1739878186297,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"4fbf1abe-86ed-4499-e126-f0f389352a84"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with C=0.5: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :-18 Write a Python program to train Logistic Regression and identify important features based on modelcoefficients.**"],"metadata":{"id":"hFbgY6Gm0lxt"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize Logistic Regression\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the model\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Get the coefficients (importance of features)\n","coefficients = log_reg.coef_[0]\n","\n","# Create a DataFrame to view the feature importance\n","feature_names = data.feature_names\n","feature_importance = pd.DataFrame({\n","    'Feature': feature_names,\n","    'Coefficient': coefficients,\n","    'Absolute Coefficient': np.abs(coefficients)\n","})\n","\n","# Sort by absolute value of coefficients (most important features first)\n","feature_importance_sorted = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n","\n","# Print feature importance\n","print(\"Feature importance based on coefficients:\")\n","print(feature_importance_sorted)\n","\n","# Evaluate model accuracy (optional)\n","accuracy = log_reg.score(X_test_scaled, y_test)\n","print(f\"\\nAccuracy of Logistic Regression: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6EGxmlH70i_4","executionInfo":{"status":"ok","timestamp":1739878260647,"user_tz":-330,"elapsed":424,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"1085cfa3-865b-417e-e6c3-ac1dc2c03275"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature importance based on coefficients:\n","             Feature  Coefficient  Absolute Coefficient\n","2  petal length (cm)    -1.672349              1.672349\n","3   petal width (cm)    -1.539707              1.539707\n","1   sepal width (cm)     1.183028              1.183028\n","0  sepal length (cm)    -0.999106              0.999106\n","\n","Accuracy of Logistic Regression: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score**"],"metadata":{"id":"i1ZPfLRX07tZ"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize Logistic Regression\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the model\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test data\n","y_pred = log_reg.predict(X_test_scaled)\n","\n","# Calculate Cohen's Kappa score\n","kappa = cohen_kappa_score(y_test, y_pred)\n","\n","# Print the Cohen's Kappa score\n","print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n","\n","# Evaluate accuracy (optional)\n","accuracy = log_reg.score(X_test_scaled, y_test)\n","print(f\"Accuracy of Logistic Regression: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yt_Vn7kK01tK","executionInfo":{"status":"ok","timestamp":1739878338277,"user_tz":-330,"elapsed":389,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"03a16ed6-d7a6-4115-d2b8-244459b081e2"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Cohen's Kappa Score: 1.0000\n","Accuracy of Logistic Regression: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio:**"],"metadata":{"id":"wxe4smZw1QLP"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize Logistic Regression\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the model\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Make probability predictions on the test data (needed for precision-recall curve)\n","y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class\n","\n","# Calculate precision and recall for different thresholds\n","precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n","\n","# Plot the Precision-Recall curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='b', label='Precision-Recall curve')\n","plt.fill_between(recall, precision, color='b', alpha=0.2)\n","plt.title(\"Precision-Recall Curve for Logistic Regression\")\n","plt.xlabel(\"Recall\")\n","plt.ylabel(\"Precision\")\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","plt.show()\n","\n","# Optionally, calculate and print the area under the Precision-Recall curve (AUC)\n","from sklearn.metrics import auc\n","pr_auc = auc(recall, precision)\n","print(f\"Area Under Precision-Recall Curve (AUC): {pr_auc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"id":"ZxOLQvBT1IrS","executionInfo":{"status":"ok","timestamp":1739878405018,"user_tz":-330,"elapsed":803,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"85fcdaf6-d217-4e5f-d3d7-8618b00b6ec1"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Area Under Precision-Recall Curve (AUC): 1.0000\n"]}]},{"cell_type":"markdown","source":["# **Que :- 20 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.**"],"metadata":{"id":"lV97gYof1c7Q"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Solvers to compare\n","solvers = ['liblinear', 'saga', 'lbfgs']\n","accuracy_results = {}\n","\n","# Train Logistic Regression with different solvers and evaluate accuracy\n","for solver in solvers:\n","    # Initialize Logistic Regression with the current solver\n","    log_reg = LogisticRegression(solver=solver, max_iter=200)\n","\n","    # Train the model\n","    log_reg.fit(X_train_scaled, y_train)\n","\n","    # Make predictions on the test data\n","    y_pred = log_reg.predict(X_test_scaled)\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_test, y_pred)\n","    accuracy_results[solver] = accuracy\n","\n","# Print the accuracy of each solver\n","print(\"Accuracy comparison of different solvers:\")\n","for solver, accuracy in accuracy_results.items():\n","    print(f\"Solver: {solver} - Accuracy: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqBS0g7n1Y2h","executionInfo":{"status":"ok","timestamp":1739878478649,"user_tz":-330,"elapsed":418,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"28f88f07-c98a-47e3-bcd7-b3966d10c16f"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy comparison of different solvers:\n","Solver: liblinear - Accuracy: 100.00%\n","Solver: saga - Accuracy: 100.00%\n","Solver: lbfgs - Accuracy: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 22 Write a Python program to train Logistic Regression and evaluate its performance using MatthewsCorrelation Coefficient (MCC).**"],"metadata":{"id":"lHJ0p-b81t_S"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.preprocessing import StandardScaler\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize Logistic Regression\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the model\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test data\n","y_pred = log_reg.predict(X_test_scaled)\n","\n","# Calculate Matthews Correlation Coefficient (MCC)\n","mcc = matthews_corrcoef(y_test, y_pred)\n","\n","# Print the MCC score\n","print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ytz8V6Vd1q7s","executionInfo":{"status":"ok","timestamp":1739878588327,"user_tz":-330,"elapsed":391,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"ac920775-edbf-4726-e85a-30336361ccb9"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Matthews Correlation Coefficient (MCC): 1.0000\n"]}]},{"cell_type":"markdown","source":["## **Que :- 23 Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.**"],"metadata":{"id":"gYIgHh-g2Q5J"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Initialize Logistic Regression\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train and evaluate the model on raw data (no scaling)\n","log_reg.fit(X_train, y_train)\n","y_pred_raw = log_reg.predict(X_test)\n","accuracy_raw = accuracy_score(y_test, y_pred_raw)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Train and evaluate the model on standardized data\n","log_reg.fit(X_train_scaled, y_train)\n","y_pred_scaled = log_reg.predict(X_test_scaled)\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","# Print the accuracy comparison\n","print(f\"Accuracy on raw data: {accuracy_raw * 100:.2f}%\")\n","print(f\"Accuracy on standardized data: {accuracy_scaled * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RE7LZ9R2FuM","executionInfo":{"status":"ok","timestamp":1739878670121,"user_tz":-330,"elapsed":412,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"9edbe35a-07b9-4a2f-b9d1-668edacded8e"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on raw data: 100.00%\n","Accuracy on standardized data: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 24 Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation**"],"metadata":{"id":"Kx76upia2f47"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize Logistic Regression\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Define the range of C values to search\n","param_grid = {\n","    'C': np.logspace(-3, 3, 7)  # Range of C from 10^-3 to 10^3\n","}\n","\n","# Initialize GridSearchCV with cross-validation\n","grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, scoring='accuracy')\n","\n","# Perform grid search to find the best C\n","grid_search.fit(X_train_scaled, y_train)\n","\n","# Get the best C and corresponding accuracy\n","best_C = grid_search.best_params_['C']\n","best_accuracy = grid_search.best_score_\n","\n","# Print the best C and the corresponding accuracy\n","print(f\"Optimal C: {best_C}\")\n","print(f\"Best Cross-Validation Accuracy: {best_accuracy * 100:.2f}%\")\n","\n","# Train the model with the optimal C on the entire training set\n","log_reg_best = LogisticRegression(C=best_C, max_iter=200)\n","log_reg_best.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test data\n","y_pred = log_reg_best.predict(X_test_scaled)\n","\n","# Evaluate the model on the test data\n","test_accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the test accuracy\n","print(f\"Test Accuracy with Optimal C: {test_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BiO0mP-2Zrf","executionInfo":{"status":"ok","timestamp":1739878735537,"user_tz":-330,"elapsed":537,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"ba9d7c0e-1e92-498f-909b-0bfa1dde9de6"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal C: 0.1\n","Best Cross-Validation Accuracy: 100.00%\n","Test Accuracy with Optimal C: 100.00%\n"]}]},{"cell_type":"markdown","source":["## **Que :- 25 Write a Python program to train LogisticRegression, save the trained model using joblib, and load it again to make predictions.**"],"metadata":{"id":"nM-iP_Nb2yA_"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","import joblib  # For saving and loading the model\n","\n","# Load the Iris dataset\n","data = load_iris()\n","X = data.data  # Features\n","y = data.target  # Target variable\n","\n","# Convert to binary classification (Setosa vs Non-Setosa)\n","y_binary = (y == 0).astype(int)  # 1 for Setosa, 0 for Non-Setosa\n","\n","# Split the data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n","\n","# Standardize the feature data (important for logistic regression)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Initialize Logistic Regression\n","log_reg = LogisticRegression(max_iter=200)\n","\n","# Train the Logistic Regression model\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Save the trained model using joblib\n","joblib.dump(log_reg, 'logistic_regression_model.joblib')\n","\n","# Save the scaler as well (useful for scaling new data in the future)\n","joblib.dump(scaler, 'scaler.joblib')\n","\n","# Make predictions on the test data using the trained model\n","y_pred = log_reg.predict(X_test_scaled)\n","\n","# Evaluate the model on the test data\n","test_accuracy = accuracy_score(y_test, y_pred)\n","\n","# Print the test accuracy\n","print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n","\n","# Load the saved model and scaler using joblib\n","loaded_log_reg = joblib.load('logistic_regression_model.joblib')\n","loaded_scaler = joblib.load('scaler.joblib')\n","\n","# Make predictions on new data (let's use the first row of the test set as an example)\n","new_data = X_test[0].reshape(1, -1)  # Reshape to match the expected input format\n","new_data_scaled = loaded_scaler.transform(new_data)  # Standardize the new data\n","\n","# Predict using the loaded model\n","new_prediction = loaded_log_reg.predict(new_data_scaled)\n","\n","# Print the prediction for the new data\n","print(f\"Prediction for the new data point: {new_prediction[0]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hIrWhZEs2pnu","executionInfo":{"status":"ok","timestamp":1739878814258,"user_tz":-330,"elapsed":677,"user":{"displayName":"Sonu parashar","userId":"10017628864761346812"}},"outputId":"5f0036eb-b999-4661-9a58-e6f7f69067df"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 100.00%\n","Prediction for the new data point: 0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mJgqDKei28zf"},"execution_count":null,"outputs":[]}]}